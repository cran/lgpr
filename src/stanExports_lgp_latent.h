// Generated by rstantools.  Do not edit by hand.

#ifndef MODELS_HPP
#define MODELS_HPP
#define STAN__SERVICES__COMMAND_HPP
#ifndef USE_STANC3
#define USE_STANC3
#endif
#include <rstan/rstaninc.hpp>
// Code generated by stanc v2.26.1-4-gd72b68b7-dirty
#include <stan/model/model_header.hpp>
namespace model_lgp_latent_namespace {
inline void validate_positive_index(const char* var_name, const char* expr,
                                    int val) {
  if (val < 1) {
    std::stringstream msg;
    msg << "Found dimension size less than one in simplex declaration"
        << "; variable=" << var_name << "; dimension size expression=" << expr
        << "; expression value=" << val;
    std::string msg_str(msg.str());
    throw std::invalid_argument(msg_str.c_str());
  }
}
inline void validate_unit_vector_index(const char* var_name, const char* expr,
                                       int val) {
  if (val <= 1) {
    std::stringstream msg;
    if (val == 1) {
      msg << "Found dimension size one in unit vector declaration."
          << " One-dimensional unit vector is discrete"
          << " but the target distribution must be continuous."
          << " variable=" << var_name << "; dimension size expression=" << expr;
    } else {
      msg << "Found dimension size less than one in unit vector declaration"
          << "; variable=" << var_name << "; dimension size expression=" << expr
          << "; expression value=" << val;
    }
    std::string msg_str(msg.str());
    throw std::invalid_argument(msg_str.c_str());
  }
}
using std::istream;
using std::string;
using std::stringstream;
using std::vector;
using std::pow;
using stan::io::dump;
using stan::math::lgamma;
using stan::model::model_base_crtp;
using stan::model::rvalue;
using stan::model::cons_list;
using stan::model::index_uni;
using stan::model::index_max;
using stan::model::index_min;
using stan::model::index_min_max;
using stan::model::index_multi;
using stan::model::index_omni;
using stan::model::nil_index_list;
using namespace stan::math;
using stan::math::pow; 
stan::math::profile_map profiles__;
static int current_statement__= 0;
static const std::vector<string> locations_array__ = {" (found before start of program)",
                                                      " (in 'lgp_latent', line 419, column 2 to column 43)",
                                                      " (in 'lgp_latent', line 420, column 2 to column 39)",
                                                      " (in 'lgp_latent', line 421, column 2 to column 38)",
                                                      " (in 'lgp_latent', line 422, column 2 to column 69)",
                                                      " (in 'lgp_latent', line 423, column 2 to column 73)",
                                                      " (in 'lgp_latent', line 424, column 2 to column 46)",
                                                      " (in 'lgp_latent', line 425, column 2 to column 44)",
                                                      " (in 'lgp_latent', line 426, column 2 to column 61)",
                                                      " (in 'lgp_latent', line 427, column 2 to column 39)",
                                                      " (in 'lgp_latent', line 430, column 2 to column 44)",
                                                      " (in 'lgp_latent', line 433, column 2 to column 41)",
                                                      " (in 'lgp_latent', line 435, column 4 to column 68)",
                                                      " (in 'lgp_latent', line 434, column 23 to line 436, column 3)",
                                                      " (in 'lgp_latent', line 434, column 2 to line 436, column 3)",
                                                      " (in 'lgp_latent', line 438, column 11 to column 18)",
                                                      " (in 'lgp_latent', line 438, column 20 to column 27)",
                                                      " (in 'lgp_latent', line 438, column 4 to column 60)",
                                                      " (in 'lgp_latent', line 439, column 10 to column 19)",
                                                      " (in 'lgp_latent', line 439, column 28 to column 35)",
                                                      " (in 'lgp_latent', line 439, column 37 to column 44)",
                                                      " (in 'lgp_latent', line 439, column 4 to line 444, column 6)",
                                                      " (in 'lgp_latent', line 446, column 6 to column 63)",
                                                      " (in 'lgp_latent', line 445, column 25 to line 447, column 5)",
                                                      " (in 'lgp_latent', line 445, column 4 to line 447, column 5)",
                                                      " (in 'lgp_latent', line 437, column 2 to line 448, column 3)",
                                                      " (in 'lgp_latent', line 451, column 9 to column 16)",
                                                      " (in 'lgp_latent', line 451, column 2 to column 68)",
                                                      " (in 'lgp_latent', line 456, column 4 to column 71)",
                                                      " (in 'lgp_latent', line 455, column 23 to line 457, column 3)",
                                                      " (in 'lgp_latent', line 455, column 2 to line 457, column 3)",
                                                      " (in 'lgp_latent', line 461, column 4 to column 65)",
                                                      " (in 'lgp_latent', line 460, column 21 to line 462, column 3)",
                                                      " (in 'lgp_latent', line 460, column 2 to line 462, column 3)",
                                                      " (in 'lgp_latent', line 466, column 4 to column 65)",
                                                      " (in 'lgp_latent', line 465, column 20 to line 467, column 3)",
                                                      " (in 'lgp_latent', line 465, column 2 to line 467, column 3)",
                                                      " (in 'lgp_latent', line 471, column 4 to column 70)",
                                                      " (in 'lgp_latent', line 470, column 23 to line 472, column 3)",
                                                      " (in 'lgp_latent', line 470, column 2 to line 472, column 3)",
                                                      " (in 'lgp_latent', line 476, column 4 to column 33)",
                                                      " (in 'lgp_latent', line 477, column 4 to column 40)",
                                                      " (in 'lgp_latent', line 478, column 4 to column 43)",
                                                      " (in 'lgp_latent', line 479, column 11 to column 17)",
                                                      " (in 'lgp_latent', line 479, column 4 to column 61)",
                                                      " (in 'lgp_latent', line 481, column 6 to column 65)",
                                                      " (in 'lgp_latent', line 480, column 22 to line 482, column 5)",
                                                      " (in 'lgp_latent', line 480, column 4 to line 482, column 5)",
                                                      " (in 'lgp_latent', line 475, column 23 to line 483, column 3)",
                                                      " (in 'lgp_latent', line 475, column 2 to line 483, column 3)",
                                                      " (in 'lgp_latent', line 484, column 25 to column 62)",
                                                      " (in 'lgp_latent', line 484, column 23 to column 64)",
                                                      " (in 'lgp_latent', line 484, column 2 to column 64)",
                                                      " (in 'lgp_latent', line 490, column 4 to column 73)",
                                                      " (in 'lgp_latent', line 489, column 24 to line 491, column 3)",
                                                      " (in 'lgp_latent', line 489, column 8 to line 491, column 3)",
                                                      " (in 'lgp_latent', line 488, column 4 to column 65)",
                                                      " (in 'lgp_latent', line 487, column 24 to line 489, column 3)",
                                                      " (in 'lgp_latent', line 487, column 8 to line 491, column 3)",
                                                      " (in 'lgp_latent', line 486, column 4 to column 71)",
                                                      " (in 'lgp_latent', line 485, column 18 to line 487, column 3)",
                                                      " (in 'lgp_latent', line 485, column 2 to line 491, column 3)",
                                                      " (in 'lgp_latent', line 512, column 4 to column 36)",
                                                      " (in 'lgp_latent', line 513, column 11 to column 18)",
                                                      " (in 'lgp_latent', line 513, column 4 to column 41)",
                                                      " (in 'lgp_latent', line 514, column 10 to column 17)",
                                                      " (in 'lgp_latent', line 514, column 4 to column 51)",
                                                      " (in 'lgp_latent', line 515, column 10 to column 17)",
                                                      " (in 'lgp_latent', line 515, column 4 to column 59)",
                                                      " (in 'lgp_latent', line 516, column 4 to column 69)",
                                                      " (in 'lgp_latent', line 510, column 52 to line 517, column 3)",
                                                      " (in 'lgp_latent', line 510, column 8 to line 517, column 3)",
                                                      " (in 'lgp_latent', line 508, column 10 to column 17)",
                                                      " (in 'lgp_latent', line 508, column 4 to column 53)",
                                                      " (in 'lgp_latent', line 509, column 4 to column 71)",
                                                      " (in 'lgp_latent', line 506, column 52 to line 510, column 3)",
                                                      " (in 'lgp_latent', line 506, column 8 to line 517, column 3)",
                                                      " (in 'lgp_latent', line 503, column 10 to column 17)",
                                                      " (in 'lgp_latent', line 503, column 4 to column 52)",
                                                      " (in 'lgp_latent', line 504, column 10 to column 17)",
                                                      " (in 'lgp_latent', line 504, column 4 to column 71)",
                                                      " (in 'lgp_latent', line 505, column 4 to column 62)",
                                                      " (in 'lgp_latent', line 501, column 52 to line 506, column 3)",
                                                      " (in 'lgp_latent', line 501, column 8 to line 517, column 3)",
                                                      " (in 'lgp_latent', line 499, column 10 to column 17)",
                                                      " (in 'lgp_latent', line 499, column 4 to column 52)",
                                                      " (in 'lgp_latent', line 500, column 4 to column 50)",
                                                      " (in 'lgp_latent', line 497, column 52 to line 501, column 3)",
                                                      " (in 'lgp_latent', line 497, column 8 to line 517, column 3)",
                                                      " (in 'lgp_latent', line 495, column 10 to column 17)",
                                                      " (in 'lgp_latent', line 495, column 4 to column 48)",
                                                      " (in 'lgp_latent', line 496, column 4 to column 52)",
                                                      " (in 'lgp_latent', line 493, column 47 to line 497, column 3)",
                                                      " (in 'lgp_latent', line 493, column 2 to line 517, column 3)",
                                                      " (in 'lgp_latent', line 322, column 2 to column 35)",
                                                      " (in 'lgp_latent', line 323, column 2 to column 46)",
                                                      " (in 'lgp_latent', line 326, column 2 to column 23)",
                                                      " (in 'lgp_latent', line 327, column 2 to column 28)",
                                                      " (in 'lgp_latent', line 328, column 2 to column 27)",
                                                      " (in 'lgp_latent', line 329, column 2 to column 25)",
                                                      " (in 'lgp_latent', line 330, column 2 to column 23)",
                                                      " (in 'lgp_latent', line 331, column 2 to column 22)",
                                                      " (in 'lgp_latent', line 332, column 2 to column 25)",
                                                      " (in 'lgp_latent', line 333, column 2 to column 25)",
                                                      " (in 'lgp_latent', line 334, column 2 to column 22)",
                                                      " (in 'lgp_latent', line 361, column 8 to column 17)",
                                                      " (in 'lgp_latent', line 361, column 2 to column 46)",
                                                      " (in 'lgp_latent', line 364, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 364, column 28 to column 34)",
                                                      " (in 'lgp_latent', line 364, column 2 to column 46)",
                                                      " (in 'lgp_latent', line 365, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 365, column 28 to column 34)",
                                                      " (in 'lgp_latent', line 365, column 2 to column 44)",
                                                      " (in 'lgp_latent', line 366, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 366, column 28 to column 34)",
                                                      " (in 'lgp_latent', line 366, column 2 to column 44)",
                                                      " (in 'lgp_latent', line 369, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 369, column 2 to column 51)",
                                                      " (in 'lgp_latent', line 370, column 2 to column 13)",
                                                      " (in 'lgp_latent', line 371, column 2 to column 26)",
                                                      " (in 'lgp_latent', line 374, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 374, column 29 to column 36)",
                                                      " (in 'lgp_latent', line 374, column 2 to column 45)",
                                                      " (in 'lgp_latent', line 375, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 375, column 29 to column 36)",
                                                      " (in 'lgp_latent', line 375, column 2 to column 52)",
                                                      " (in 'lgp_latent', line 376, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 376, column 22 to column 29)",
                                                      " (in 'lgp_latent', line 376, column 2 to column 47)",
                                                      " (in 'lgp_latent', line 377, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 377, column 21 to column 28)",
                                                      " (in 'lgp_latent', line 377, column 2 to column 40)",
                                                      " (in 'lgp_latent', line 383, column 8 to column 15)",
                                                      " (in 'lgp_latent', line 383, column 2 to column 57)",
                                                      " (in 'lgp_latent', line 386, column 8 to column 17)",
                                                      " (in 'lgp_latent', line 386, column 2 to column 47)",
                                                      " (in 'lgp_latent', line 387, column 8 to column 15)",
                                                      " (in 'lgp_latent', line 387, column 2 to column 43)",
                                                      " (in 'lgp_latent', line 388, column 8 to column 14)",
                                                      " (in 'lgp_latent', line 388, column 2 to column 42)",
                                                      " (in 'lgp_latent', line 389, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 389, column 2 to column 48)",
                                                      " (in 'lgp_latent', line 390, column 8 to column 17)",
                                                      " (in 'lgp_latent', line 390, column 2 to column 39)",
                                                      " (in 'lgp_latent', line 391, column 8 to column 15)",
                                                      " (in 'lgp_latent', line 391, column 2 to column 35)",
                                                      " (in 'lgp_latent', line 392, column 8 to column 14)",
                                                      " (in 'lgp_latent', line 392, column 2 to column 34)",
                                                      " (in 'lgp_latent', line 393, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 393, column 2 to column 40)",
                                                      " (in 'lgp_latent', line 394, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 394, column 2 to column 40)",
                                                      " (in 'lgp_latent', line 395, column 2 to column 33)",
                                                      " (in 'lgp_latent', line 396, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 396, column 21 to column 28)",
                                                      " (in 'lgp_latent', line 396, column 2 to column 49)",
                                                      " (in 'lgp_latent', line 397, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 397, column 22 to column 29)",
                                                      " (in 'lgp_latent', line 397, column 2 to column 43)",
                                                      " (in 'lgp_latent', line 398, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 398, column 21 to column 28)",
                                                      " (in 'lgp_latent', line 398, column 2 to column 56)",
                                                      " (in 'lgp_latent', line 399, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 399, column 2 to column 50)",
                                                      " (in 'lgp_latent', line 400, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 400, column 2 to column 48)",
                                                      " (in 'lgp_latent', line 401, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 401, column 2 to column 42)",
                                                      " (in 'lgp_latent', line 402, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 402, column 2 to column 40)",
                                                      " (in 'lgp_latent', line 403, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 403, column 2 to column 42)",
                                                      " (in 'lgp_latent', line 404, column 9 to column 16)",
                                                      " (in 'lgp_latent', line 404, column 2 to column 24)",
                                                      " (in 'lgp_latent', line 409, column 8 to column 17)",
                                                      " (in 'lgp_latent', line 409, column 26 to column 33)",
                                                      " (in 'lgp_latent', line 409, column 35 to column 42)",
                                                      " (in 'lgp_latent', line 409, column 2 to line 412, column 4)",
                                                      " (in 'lgp_latent', line 415, column 9 to column 16)",
                                                      " (in 'lgp_latent', line 415, column 2 to column 57)",
                                                      " (in 'lgp_latent', line 419, column 8 to column 17)",
                                                      " (in 'lgp_latent', line 420, column 8 to column 15)",
                                                      " (in 'lgp_latent', line 421, column 8 to column 14)",
                                                      " (in 'lgp_latent', line 422, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 422, column 56 to column 62)",
                                                      " (in 'lgp_latent', line 423, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 423, column 56 to column 62)",
                                                      " (in 'lgp_latent', line 424, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 425, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 426, column 8 to column 20)",
                                                      " (in 'lgp_latent', line 427, column 8 to column 17)",
                                                      " (in 'lgp_latent', line 427, column 26 to column 33)",
                                                      " (in 'lgp_latent', line 430, column 8 to column 17)",
                                                      " (in 'lgp_latent', line 430, column 26 to column 33)",
                                                      " (in 'lgp_latent', line 433, column 8 to column 19)",
                                                      " (in 'lgp_latent', line 433, column 28 to column 34)",
                                                      " (in 'lgp_latent', line 21, column 4 to column 30)",
                                                      " (in 'lgp_latent', line 22, column 11 to column 12)",
                                                      " (in 'lgp_latent', line 22, column 4 to column 35)",
                                                      " (in 'lgp_latent', line 24, column 6 to column 19)",
                                                      " (in 'lgp_latent', line 23, column 25 to line 25, column 5)",
                                                      " (in 'lgp_latent', line 23, column 4 to line 25, column 5)",
                                                      " (in 'lgp_latent', line 26, column 4 to column 14)",
                                                      " (in 'lgp_latent', line 20, column 56 to line 27, column 3)",
                                                      " (in 'lgp_latent', line 31, column 4 to column 24)",
                                                      " (in 'lgp_latent', line 32, column 4 to column 24)",
                                                      " (in 'lgp_latent', line 33, column 11 to column 13)",
                                                      " (in 'lgp_latent', line 33, column 15 to column 17)",
                                                      " (in 'lgp_latent', line 33, column 4 to column 32)",
                                                      " (in 'lgp_latent', line 35, column 6 to column 20)",
                                                      " (in 'lgp_latent', line 34, column 23 to line 36, column 5)",
                                                      " (in 'lgp_latent', line 34, column 4 to line 36, column 5)",
                                                      " (in 'lgp_latent', line 37, column 4 to column 18)",
                                                      " (in 'lgp_latent', line 30, column 48 to line 38, column 3)",
                                                      " (in 'lgp_latent', line 42, column 4 to column 38)",
                                                      " (in 'lgp_latent', line 41, column 42 to line 43, column 3)",
                                                      " (in 'lgp_latent', line 47, column 4 to column 31)",
                                                      " (in 'lgp_latent', line 46, column 40 to line 48, column 3)",
                                                      " (in 'lgp_latent', line 52, column 4 to column 28)",
                                                      " (in 'lgp_latent', line 53, column 11 to column 14)",
                                                      " (in 'lgp_latent', line 53, column 4 to column 46)",
                                                      " (in 'lgp_latent', line 54, column 4 to column 24)",
                                                      " (in 'lgp_latent', line 55, column 4 to column 31)",
                                                      " (in 'lgp_latent', line 51, column 59 to line 56, column 3)",
                                                      " (in 'lgp_latent', line 65, column 4 to column 33)",
                                                      " (in 'lgp_latent', line 66, column 11 to column 12)",
                                                      " (in 'lgp_latent', line 66, column 4 to column 61)",
                                                      " (in 'lgp_latent', line 67, column 11 to column 12)",
                                                      " (in 'lgp_latent', line 67, column 4 to column 53)",
                                                      " (in 'lgp_latent', line 68, column 4 to column 41)",
                                                      " (in 'lgp_latent', line 64, column 2 to line 69, column 3)",
                                                      " (in 'lgp_latent', line 73, column 4 to column 23)",
                                                      " (in 'lgp_latent', line 74, column 4 to column 15)",
                                                      " (in 'lgp_latent', line 78, column 6 to column 33)",
                                                      " (in 'lgp_latent', line 79, column 6 to column 20)",
                                                      " (in 'lgp_latent', line 77, column 20 to line 80, column 5)",
                                                      " (in 'lgp_latent', line 77, column 4 to line 80, column 5)",
                                                      " (in 'lgp_latent', line 92, column 6 to column 50)",
                                                      " (in 'lgp_latent', line 91, column 26 to line 93, column 5)",
                                                      " (in 'lgp_latent', line 91, column 10 to line 93, column 5)",
                                                      " (in 'lgp_latent', line 90, column 6 to column 50)",
                                                      " (in 'lgp_latent', line 89, column 26 to line 91, column 5)",
                                                      " (in 'lgp_latent', line 89, column 10 to line 93, column 5)",
                                                      " (in 'lgp_latent', line 88, column 6 to column 46)",
                                                      " (in 'lgp_latent', line 87, column 26 to line 89, column 5)",
                                                      " (in 'lgp_latent', line 87, column 10 to line 93, column 5)",
                                                      " (in 'lgp_latent', line 86, column 6 to column 54)",
                                                      " (in 'lgp_latent', line 85, column 26 to line 87, column 5)",
                                                      " (in 'lgp_latent', line 85, column 10 to line 93, column 5)",
                                                      " (in 'lgp_latent', line 84, column 6 to column 47)",
                                                      " (in 'lgp_latent', line 83, column 20 to line 85, column 5)",
                                                      " (in 'lgp_latent', line 83, column 4 to line 93, column 5)",
                                                      " (in 'lgp_latent', line 95, column 4 to column 22)",
                                                      " (in 'lgp_latent', line 72, column 75 to line 96, column 3)",
                                                      " (in 'lgp_latent', line 100, column 4 to column 22)",
                                                      " (in 'lgp_latent', line 101, column 4 to column 22)",
                                                      " (in 'lgp_latent', line 102, column 11 to column 13)",
                                                      " (in 'lgp_latent', line 102, column 15 to column 17)",
                                                      " (in 'lgp_latent', line 102, column 4 to column 21)",
                                                      " (in 'lgp_latent', line 108, column 10 to column 38)",
                                                      " (in 'lgp_latent', line 107, column 15 to line 109, column 9)",
                                                      " (in 'lgp_latent', line 106, column 10 to column 21)",
                                                      " (in 'lgp_latent', line 105, column 28 to line 107, column 9)",
                                                      " (in 'lgp_latent', line 105, column 8 to line 109, column 9)",
                                                      " (in 'lgp_latent', line 104, column 22 to line 110, column 7)",
                                                      " (in 'lgp_latent', line 104, column 6 to line 110, column 7)",
                                                      " (in 'lgp_latent', line 103, column 20 to line 111, column 5)",
                                                      " (in 'lgp_latent', line 103, column 4 to line 111, column 5)",
                                                      " (in 'lgp_latent', line 112, column 4 to column 14)",
                                                      " (in 'lgp_latent', line 99, column 89 to line 113, column 3)",
                                                      " (in 'lgp_latent', line 117, column 4 to column 22)",
                                                      " (in 'lgp_latent', line 118, column 4 to column 22)",
                                                      " (in 'lgp_latent', line 119, column 11 to column 13)",
                                                      " (in 'lgp_latent', line 119, column 14 to column 16)",
                                                      " (in 'lgp_latent', line 119, column 4 to column 20)",
                                                      " (in 'lgp_latent', line 122, column 8 to column 34)",
                                                      " (in 'lgp_latent', line 121, column 22 to line 123, column 7)",
                                                      " (in 'lgp_latent', line 121, column 6 to line 123, column 7)",
                                                      " (in 'lgp_latent', line 120, column 20 to line 124, column 5)",
                                                      " (in 'lgp_latent', line 120, column 4 to line 124, column 5)",
                                                      " (in 'lgp_latent', line 125, column 4 to column 14)",
                                                      " (in 'lgp_latent', line 116, column 67 to line 126, column 3)",
                                                      " (in 'lgp_latent', line 130, column 4 to column 22)",
                                                      " (in 'lgp_latent', line 131, column 4 to column 22)",
                                                      " (in 'lgp_latent', line 132, column 11 to column 13)",
                                                      " (in 'lgp_latent', line 132, column 14 to column 16)",
                                                      " (in 'lgp_latent', line 132, column 4 to column 20)",
                                                      " (in 'lgp_latent', line 135, column 8 to column 45)",
                                                      " (in 'lgp_latent', line 134, column 22 to line 136, column 7)",
                                                      " (in 'lgp_latent', line 134, column 6 to line 136, column 7)",
                                                      " (in 'lgp_latent', line 133, column 20 to line 137, column 5)",
                                                      " (in 'lgp_latent', line 133, column 4 to line 137, column 5)",
                                                      " (in 'lgp_latent', line 138, column 4 to column 14)",
                                                      " (in 'lgp_latent', line 129, column 67 to line 139, column 3)",
                                                      " (in 'lgp_latent', line 146, column 4 to column 30)",
                                                      " (in 'lgp_latent', line 147, column 4 to column 30)",
                                                      " (in 'lgp_latent', line 148, column 11 to column 13)",
                                                      " (in 'lgp_latent', line 148, column 15 to column 17)",
                                                      " (in 'lgp_latent', line 148, column 4 to column 21)",
                                                      " (in 'lgp_latent', line 155, column 6 to column 44)",
                                                      " (in 'lgp_latent', line 153, column 11 to line 156, column 5)",
                                                      " (in 'lgp_latent', line 152, column 6 to column 34)",
                                                      " (in 'lgp_latent', line 151, column 33 to line 153, column 5)",
                                                      " (in 'lgp_latent', line 151, column 11 to line 156, column 5)",
                                                      " (in 'lgp_latent', line 150, column 6 to column 34)",
                                                      " (in 'lgp_latent', line 149, column 26 to line 151, column 5)",
                                                      " (in 'lgp_latent', line 149, column 4 to line 156, column 5)",
                                                      " (in 'lgp_latent', line 157, column 4 to column 14)",
                                                      " (in 'lgp_latent', line 145, column 2 to line 158, column 3)",
                                                      " (in 'lgp_latent', line 168, column 4 to column 37)",
                                                      " (in 'lgp_latent', line 169, column 10 to column 19)",
                                                      " (in 'lgp_latent', line 169, column 28 to column 30)",
                                                      " (in 'lgp_latent', line 169, column 32 to column 34)",
                                                      " (in 'lgp_latent', line 169, column 4 to column 44)",
                                                      " (in 'lgp_latent', line 171, column 13 to column 15)",
                                                      " (in 'lgp_latent', line 171, column 17 to column 19)",
                                                      " (in 'lgp_latent', line 171, column 6 to column 23)",
                                                      " (in 'lgp_latent', line 172, column 6 to column 40)",
                                                      " (in 'lgp_latent', line 173, column 6 to column 26)",
                                                      " (in 'lgp_latent', line 174, column 6 to column 26)",
                                                      " (in 'lgp_latent', line 175, column 6 to column 28)",
                                                      " (in 'lgp_latent', line 176, column 6 to column 29)",
                                                      " (in 'lgp_latent', line 182, column 8 to column 34)",
                                                      " (in 'lgp_latent', line 181, column 13 to line 183, column 7)",
                                                      " (in 'lgp_latent', line 180, column 8 to column 74)",
                                                      " (in 'lgp_latent', line 179, column 25 to line 181, column 7)",
                                                      " (in 'lgp_latent', line 179, column 6 to line 183, column 7)",
                                                      " (in 'lgp_latent', line 187, column 8 to column 36)",
                                                      " (in 'lgp_latent', line 188, column 8 to column 71)",
                                                      " (in 'lgp_latent', line 186, column 36 to line 189, column 7)",
                                                      " (in 'lgp_latent', line 186, column 6 to line 189, column 7)",
                                                      " (in 'lgp_latent', line 190, column 6 to column 21)",
                                                      " (in 'lgp_latent', line 170, column 27 to line 191, column 5)",
                                                      " (in 'lgp_latent', line 170, column 4 to line 191, column 5)",
                                                      " (in 'lgp_latent', line 192, column 4 to column 20)",
                                                      " (in 'lgp_latent', line 167, column 2 to line 193, column 3)",
                                                      " (in 'lgp_latent', line 197, column 4 to column 74)",
                                                      " (in 'lgp_latent', line 196, column 68 to line 198, column 3)",
                                                      " (in 'lgp_latent', line 204, column 4 to column 38)",
                                                      " (in 'lgp_latent', line 205, column 4 to column 40)",
                                                      " (in 'lgp_latent', line 206, column 4 to line 209, column 6)",
                                                      " (in 'lgp_latent', line 203, column 2 to line 210, column 3)",
                                                      " (in 'lgp_latent', line 214, column 4 to line 217, column 6)",
                                                      " (in 'lgp_latent', line 213, column 89 to line 218, column 3)",
                                                      " (in 'lgp_latent', line 243, column 4 to column 20)",
                                                      " (in 'lgp_latent', line 244, column 4 to column 20)",
                                                      " (in 'lgp_latent', line 245, column 4 to column 22)",
                                                      " (in 'lgp_latent', line 246, column 4 to column 37)",
                                                      " (in 'lgp_latent', line 247, column 10 to column 19)",
                                                      " (in 'lgp_latent', line 247, column 28 to column 30)",
                                                      " (in 'lgp_latent', line 247, column 32 to column 34)",
                                                      " (in 'lgp_latent', line 247, column 4 to column 39)",
                                                      " (in 'lgp_latent', line 253, column 13 to column 15)",
                                                      " (in 'lgp_latent', line 253, column 17 to column 19)",
                                                      " (in 'lgp_latent', line 253, column 6 to column 36)",
                                                      " (in 'lgp_latent', line 254, column 13 to column 15)",
                                                      " (in 'lgp_latent', line 254, column 6 to column 20)",
                                                      " (in 'lgp_latent', line 255, column 13 to column 15)",
                                                      " (in 'lgp_latent', line 255, column 6 to column 20)",
                                                      " (in 'lgp_latent', line 258, column 6 to column 40)",
                                                      " (in 'lgp_latent', line 259, column 6 to column 26)",
                                                      " (in 'lgp_latent', line 260, column 6 to column 29)",
                                                      " (in 'lgp_latent', line 261, column 6 to column 29)",
                                                      " (in 'lgp_latent', line 262, column 6 to column 30)",
                                                      " (in 'lgp_latent', line 263, column 6 to column 34)",
                                                      " (in 'lgp_latent', line 264, column 6 to column 29)",
                                                      " (in 'lgp_latent', line 272, column 10 to column 28)",
                                                      " (in 'lgp_latent', line 273, column 10 to column 28)",
                                                      " (in 'lgp_latent', line 271, column 13 to line 274, column 9)",
                                                      " (in 'lgp_latent', line 269, column 10 to column 35)",
                                                      " (in 'lgp_latent', line 270, column 10 to column 35)",
                                                      " (in 'lgp_latent', line 268, column 21 to line 271, column 9)",
                                                      " (in 'lgp_latent', line 268, column 8 to line 274, column 9)",
                                                      " (in 'lgp_latent', line 267, column 20 to line 275, column 7)",
                                                      " (in 'lgp_latent', line 267, column 6 to line 275, column 7)",
                                                      " (in 'lgp_latent', line 279, column 8 to column 15)",
                                                      " (in 'lgp_latent', line 280, column 8 to column 21)",
                                                      " (in 'lgp_latent', line 284, column 10 to column 72)",
                                                      " (in 'lgp_latent', line 285, column 10 to column 72)",
                                                      " (in 'lgp_latent', line 283, column 20 to line 286, column 9)",
                                                      " (in 'lgp_latent', line 283, column 8 to line 286, column 9)",
                                                      " (in 'lgp_latent', line 289, column 8 to column 25)",
                                                      " (in 'lgp_latent', line 291, column 10 to column 61)",
                                                      " (in 'lgp_latent', line 290, column 25 to line 292, column 9)",
                                                      " (in 'lgp_latent', line 290, column 8 to line 292, column 9)",
                                                      " (in 'lgp_latent', line 295, column 8 to column 36)",
                                                      " (in 'lgp_latent', line 296, column 8 to column 36)",
                                                      " (in 'lgp_latent', line 278, column 19 to line 297, column 7)",
                                                      " (in 'lgp_latent', line 278, column 6 to line 297, column 7)",
                                                      " (in 'lgp_latent', line 300, column 6 to column 21)",
                                                      " (in 'lgp_latent', line 305, column 8 to column 41)",
                                                      " (in 'lgp_latent', line 304, column 13 to line 306, column 7)",
                                                      " (in 'lgp_latent', line 302, column 8 to column 21)",
                                                      " (in 'lgp_latent', line 303, column 8 to column 72)",
                                                      " (in 'lgp_latent', line 301, column 20 to line 304, column 7)",
                                                      " (in 'lgp_latent', line 301, column 6 to line 306, column 7)",
                                                      " (in 'lgp_latent', line 310, column 8 to column 69)",
                                                      " (in 'lgp_latent', line 309, column 18 to line 311, column 7)",
                                                      " (in 'lgp_latent', line 309, column 6 to line 311, column 7)",
                                                      " (in 'lgp_latent', line 313, column 6 to column 16)",
                                                      " (in 'lgp_latent', line 250, column 25 to line 314, column 5)",
                                                      " (in 'lgp_latent', line 250, column 4 to line 314, column 5)",
                                                      " (in 'lgp_latent', line 316, column 4 to column 15)",
                                                      " (in 'lgp_latent', line 242, column 2 to line 317, column 3)"};
template <typename T0__>
Eigen::Matrix<stan::promote_args_t<T0__>, -1, 1>
STAN_vectorsum(const std::vector<Eigen::Matrix<T0__, -1, 1>>& vecs,
               const int& L, std::ostream* pstream__) {
  using local_scalar_t__ = stan::promote_args_t<T0__>;
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    int num_vecs;
    num_vecs = std::numeric_limits<int>::min();
    
    current_statement__ = 196;
    num_vecs = stan::math::size(vecs);
    current_statement__ = 197;
    validate_non_negative_index("s", "L", L);
    Eigen::Matrix<local_scalar_t__, -1, 1> s;
    s = Eigen::Matrix<local_scalar_t__, -1, 1>(L);
    stan::math::fill(s, DUMMY_VAR__);
    
    current_statement__ = 198;
    assign(s, nil_index_list(), rep_vector(0, L), "assigning variable s");
    current_statement__ = 201;
    for (int j = 1; j <= num_vecs; ++j) {
      current_statement__ = 199;
      assign(s, nil_index_list(),
        add(stan::model::deep_copy(s), vecs[(j - 1)]), "assigning variable s");
    }
    current_statement__ = 202;
    return s;
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_vectorsum_functor__ {
template <typename T0__>
Eigen::Matrix<stan::promote_args_t<T0__>, -1, 1>
operator()(const std::vector<Eigen::Matrix<T0__, -1, 1>>& vecs, const int& L,
           std::ostream* pstream__)  const 
{
return STAN_vectorsum(vecs, L, pstream__);
}
};
template <typename T0__>
Eigen::Matrix<stan::promote_args_t<T0__>, -1, -1>
STAN_matrix_array_sum(const std::vector<Eigen::Matrix<T0__, -1, -1>>& K,
                      std::ostream* pstream__) {
  using local_scalar_t__ = stan::promote_args_t<T0__>;
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    int n1;
    n1 = std::numeric_limits<int>::min();
    
    current_statement__ = 204;
    n1 = rows(K[(1 - 1)]);
    int n2;
    n2 = std::numeric_limits<int>::min();
    
    current_statement__ = 205;
    n2 = cols(K[(1 - 1)]);
    current_statement__ = 206;
    validate_non_negative_index("K_sum", "n1", n1);
    current_statement__ = 207;
    validate_non_negative_index("K_sum", "n2", n2);
    Eigen::Matrix<local_scalar_t__, -1, -1> K_sum;
    K_sum = Eigen::Matrix<local_scalar_t__, -1, -1>(n1, n2);
    stan::math::fill(K_sum, DUMMY_VAR__);
    
    current_statement__ = 208;
    assign(K_sum, nil_index_list(), K[(1 - 1)], "assigning variable K_sum");
    current_statement__ = 211;
    for (int j = 2; j <= stan::math::size(K); ++j) {
      current_statement__ = 209;
      assign(K_sum, nil_index_list(),
        add(stan::model::deep_copy(K_sum), K[(j - 1)]),
        "assigning variable K_sum");}
    current_statement__ = 212;
    return K_sum;
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_matrix_array_sum_functor__ {
template <typename T0__>
Eigen::Matrix<stan::promote_args_t<T0__>, -1, -1>
operator()(const std::vector<Eigen::Matrix<T0__, -1, -1>>& K,
           std::ostream* pstream__)  const 
{
return STAN_matrix_array_sum(K, pstream__);
}
};
template <typename T0__, typename T1__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>,
T1__>, -1, 1>
STAN_warp_input(const T0__& x_arg__, const T1__& a, std::ostream* pstream__) {
  using local_scalar_t__ = stan::promote_args_t<stan::value_type_t<T0__>,
          T1__>;
  const auto& x = to_ref(x_arg__);
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    current_statement__ = 214;
    return add(-1,
             multiply(2, inv(add(1, stan::math::exp(multiply(-a, x))))));
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_warp_input_functor__ {
template <typename T0__, typename T1__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>,
T1__>, -1, 1>
operator()(const T0__& x, const T1__& a, std::ostream* pstream__)  const 
{
return STAN_warp_input(x, a, pstream__);
}
};
template <typename T0__, typename T1__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>,
T1__>, -1, 1>
STAN_var_mask(const T0__& x_arg__, const T1__& a, std::ostream* pstream__) {
  using local_scalar_t__ = stan::promote_args_t<stan::value_type_t<T0__>,
          T1__>;
  const auto& x = to_ref(x_arg__);
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    current_statement__ = 216;
    return inv(add(1, stan::math::exp(multiply(-a, x))));
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_var_mask_functor__ {
template <typename T0__, typename T1__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>,
T1__>, -1, 1>
operator()(const T0__& x, const T1__& a, std::ostream* pstream__)  const 
{
return STAN_var_mask(x, a, pstream__);
}
};
template <typename T0__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>>, -1, 1>
STAN_expand(const T0__& v_arg__, const std::vector<int>& idx_expand,
            std::ostream* pstream__) {
  using local_scalar_t__ = stan::promote_args_t<stan::value_type_t<T0__>>;
  const auto& v = to_ref(v_arg__);
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    int L;
    L = std::numeric_limits<int>::min();
    
    current_statement__ = 218;
    L = num_elements(v);
    current_statement__ = 219;
    validate_non_negative_index("v_add0", "L + 1", (L + 1));
    Eigen::Matrix<local_scalar_t__, -1, 1> v_add0;
    v_add0 = Eigen::Matrix<local_scalar_t__, -1, 1>((L + 1));
    stan::math::fill(v_add0, DUMMY_VAR__);
    
    current_statement__ = 220;
    assign(v_add0, nil_index_list(), rep_vector(0.0, (L + 1)),
      "assigning variable v_add0");
    current_statement__ = 221;
    assign(v_add0, cons_list(index_min_max(2, (L + 1)), nil_index_list()), v,
      "assigning variable v_add0");
    current_statement__ = 222;
    return rvalue(v_add0,
             cons_list(index_multi(idx_expand), nil_index_list()), "v_add0");
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_expand_functor__ {
template <typename T0__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>>, -1, 1>
operator()(const T0__& v, const std::vector<int>& idx_expand,
           std::ostream* pstream__)  const 
{
return STAN_expand(v, idx_expand, pstream__);
}
};
template <typename T0__, typename T2__, typename T3__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>, stan::value_type_t<T2__>,
stan::value_type_t<T3__>>, -1, 1>
STAN_edit_x_cont(const T0__& x_cont_arg__,
                 const std::vector<int>& idx_expand,
                 const T2__& teff_obs_arg__, const T3__& teff_arg__,
                 std::ostream* pstream__) {
  using local_scalar_t__ = stan::promote_args_t<stan::value_type_t<T0__>,
          stan::value_type_t<T2__>,
          stan::value_type_t<T3__>>;
  const auto& x_cont = to_ref(x_cont_arg__);
  const auto& teff_obs = to_ref(teff_obs_arg__);
  const auto& teff = to_ref(teff_arg__);
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    int n;
    n = std::numeric_limits<int>::min();
    
    current_statement__ = 224;
    n = num_elements(x_cont);
    current_statement__ = 225;
    validate_non_negative_index("x_teff_obs", "n", n);
    Eigen::Matrix<local_scalar_t__, -1, 1> x_teff_obs;
    x_teff_obs = Eigen::Matrix<local_scalar_t__, -1, 1>(n);
    stan::math::fill(x_teff_obs, DUMMY_VAR__);
    
    current_statement__ = 226;
    assign(x_teff_obs, nil_index_list(),
      STAN_expand(teff_obs, idx_expand, pstream__),
      "assigning variable x_teff_obs");
    current_statement__ = 227;
    validate_non_negative_index("x_teff", "n", n);
    Eigen::Matrix<local_scalar_t__, -1, 1> x_teff;
    x_teff = Eigen::Matrix<local_scalar_t__, -1, 1>(n);
    stan::math::fill(x_teff, DUMMY_VAR__);
    
    current_statement__ = 228;
    assign(x_teff, nil_index_list(),
      STAN_expand(teff, idx_expand, pstream__), "assigning variable x_teff");
    current_statement__ = 229;
    return subtract(add(x_cont, x_teff_obs), x_teff);
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_edit_x_cont_functor__ {
template <typename T0__, typename T2__, typename T3__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>, stan::value_type_t<T2__>,
stan::value_type_t<T3__>>, -1, 1>
operator()(const T0__& x_cont, const std::vector<int>& idx_expand,
           const T2__& teff_obs, const T3__& teff, std::ostream* pstream__)  const 
{
return STAN_edit_x_cont(x_cont, idx_expand, teff_obs, teff, pstream__);
}
};
template <typename T0__>
stan::promote_args_t<T0__>
STAN_log_prior(const T0__& x, const std::vector<int>& types,
               const std::vector<double>& p, std::ostream* pstream__) {
  using local_scalar_t__ = stan::promote_args_t<T0__>;
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    local_scalar_t__ log_prior;
    log_prior = DUMMY_VAR__;
    
    current_statement__ = 231;
    log_prior = 0;
    local_scalar_t__ t;
    t = DUMMY_VAR__;
    
    current_statement__ = 232;
    t = x;
    current_statement__ = 236;
    if (logical_eq(types[(2 - 1)], 1)) {
      current_statement__ = 233;
      log_prior = (log_prior + stan::math::log(stan::math::abs((2 * x))));
      current_statement__ = 234;
      t = square(x);
    } 
    current_statement__ = 251;
    if (logical_eq(types[(1 - 1)], 2)) {
      current_statement__ = 249;
      log_prior = (log_prior + normal_lpdf<false>(t, p[(1 - 1)], p[(2 - 1)]));
    } else {
      current_statement__ = 248;
      if (logical_eq(types[(1 - 1)], 3)) {
        current_statement__ = 246;
        log_prior = (log_prior +
                      student_t_lpdf<false>(t, p[(1 - 1)], 0.0, 1.0));
      } else {
        current_statement__ = 245;
        if (logical_eq(types[(1 - 1)], 4)) {
          current_statement__ = 243;
          log_prior = (log_prior +
                        gamma_lpdf<false>(t, p[(1 - 1)], p[(2 - 1)]));
        } else {
          current_statement__ = 242;
          if (logical_eq(types[(1 - 1)], 5)) {
            current_statement__ = 240;
            log_prior = (log_prior +
                          inv_gamma_lpdf<false>(t, p[(1 - 1)], p[(2 - 1)]));
          } else {
            current_statement__ = 239;
            if (logical_eq(types[(1 - 1)], 6)) {
              current_statement__ = 237;
              log_prior = (log_prior +
                            lognormal_lpdf<false>(t, p[(1 - 1)], p[(2 - 1)]));
            } 
          }
        }
      }
    }
    current_statement__ = 252;
    return log_prior;
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_log_prior_functor__ {
template <typename T0__>
stan::promote_args_t<T0__>
operator()(const T0__& x, const std::vector<int>& types,
           const std::vector<double>& p, std::ostream* pstream__)  const 
{
return STAN_log_prior(x, types, p, pstream__);
}
};
Eigen::Matrix<double, -1, -1>
STAN_kernel_zerosum(const std::vector<int>& x1, const std::vector<int>& x2,
                    const int& num_cat, std::ostream* pstream__) {
  using local_scalar_t__ = double;
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    int n1;
    n1 = std::numeric_limits<int>::min();
    
    current_statement__ = 254;
    n1 = stan::math::size(x1);
    int n2;
    n2 = std::numeric_limits<int>::min();
    
    current_statement__ = 255;
    n2 = stan::math::size(x2);
    current_statement__ = 256;
    validate_non_negative_index("K", "n1", n1);
    current_statement__ = 257;
    validate_non_negative_index("K", "n2", n2);
    Eigen::Matrix<local_scalar_t__, -1, -1> K;
    K = Eigen::Matrix<local_scalar_t__, -1, -1>(n1, n2);
    stan::math::fill(K, DUMMY_VAR__);
    
    current_statement__ = 267;
    for (int i = 1; i <= n1; ++i) {
      current_statement__ = 265;
      for (int j = 1; j <= n2; ++j) {
        current_statement__ = 263;
        if (logical_eq(x1[(i - 1)], x2[(j - 1)])) {
          current_statement__ = 261;
          assign(K,
            cons_list(index_uni(i),
              cons_list(index_uni(j), nil_index_list())), 1,
            "assigning variable K");
        } else {
          current_statement__ = 259;
          assign(K,
            cons_list(index_uni(i),
              cons_list(index_uni(j), nil_index_list())),
            -inv((num_cat - 1)), "assigning variable K");
        }}}
    current_statement__ = 268;
    return K;
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_kernel_zerosum_functor__ {
Eigen::Matrix<double, -1, -1>
operator()(const std::vector<int>& x1, const std::vector<int>& x2,
           const int& num_cat, std::ostream* pstream__)  const 
{
return STAN_kernel_zerosum(x1, x2, num_cat, pstream__);
}
};
Eigen::Matrix<double, -1, -1>
STAN_kernel_cat(const std::vector<int>& x1, const std::vector<int>& x2,
                std::ostream* pstream__) {
  using local_scalar_t__ = double;
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    int n1;
    n1 = std::numeric_limits<int>::min();
    
    current_statement__ = 270;
    n1 = stan::math::size(x1);
    int n2;
    n2 = std::numeric_limits<int>::min();
    
    current_statement__ = 271;
    n2 = stan::math::size(x2);
    current_statement__ = 272;
    validate_non_negative_index("K", "n1", n1);
    current_statement__ = 273;
    validate_non_negative_index("K", "n2", n2);
    Eigen::Matrix<local_scalar_t__, -1, -1> K;
    K = Eigen::Matrix<local_scalar_t__, -1, -1>(n1, n2);
    stan::math::fill(K, DUMMY_VAR__);
    
    current_statement__ = 279;
    for (int i = 1; i <= n1; ++i) {
      current_statement__ = 277;
      for (int j = 1; j <= n2; ++j) {
        current_statement__ = 275;
        assign(K,
          cons_list(index_uni(i), cons_list(index_uni(j), nil_index_list())),
          logical_eq(x1[(i - 1)], x2[(j - 1)]), "assigning variable K");}}
    current_statement__ = 280;
    return K;
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_kernel_cat_functor__ {
Eigen::Matrix<double, -1, -1>
operator()(const std::vector<int>& x1, const std::vector<int>& x2,
           std::ostream* pstream__)  const 
{
return STAN_kernel_cat(x1, x2, pstream__);
}
};
Eigen::Matrix<double, -1, -1>
STAN_kernel_bin(const std::vector<int>& x1, const std::vector<int>& x2,
                std::ostream* pstream__) {
  using local_scalar_t__ = double;
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    int n1;
    n1 = std::numeric_limits<int>::min();
    
    current_statement__ = 282;
    n1 = stan::math::size(x1);
    int n2;
    n2 = std::numeric_limits<int>::min();
    
    current_statement__ = 283;
    n2 = stan::math::size(x2);
    current_statement__ = 284;
    validate_non_negative_index("K", "n1", n1);
    current_statement__ = 285;
    validate_non_negative_index("K", "n2", n2);
    Eigen::Matrix<local_scalar_t__, -1, -1> K;
    K = Eigen::Matrix<local_scalar_t__, -1, -1>(n1, n2);
    stan::math::fill(K, DUMMY_VAR__);
    
    current_statement__ = 291;
    for (int i = 1; i <= n1; ++i) {
      current_statement__ = 289;
      for (int j = 1; j <= n2; ++j) {
        current_statement__ = 287;
        assign(K,
          cons_list(index_uni(i), cons_list(index_uni(j), nil_index_list())),
          (logical_eq(x1[(i - 1)], 0) * logical_eq(x2[(j - 1)], 0)),
          "assigning variable K");}}
    current_statement__ = 292;
    return K;
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_kernel_bin_functor__ {
Eigen::Matrix<double, -1, -1>
operator()(const std::vector<int>& x1, const std::vector<int>& x2,
           std::ostream* pstream__)  const 
{
return STAN_kernel_bin(x1, x2, pstream__);
}
};
Eigen::Matrix<double, -1, -1>
STAN_kernel_const(const std::vector<int>& x1, const std::vector<int>& x2,
                  const int& kernel_type, const int& ncat,
                  std::ostream* pstream__) {
  using local_scalar_t__ = double;
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    int n1;
    n1 = std::numeric_limits<int>::min();
    
    current_statement__ = 294;
    n1 = num_elements(x1);
    int n2;
    n2 = std::numeric_limits<int>::min();
    
    current_statement__ = 295;
    n2 = num_elements(x2);
    current_statement__ = 296;
    validate_non_negative_index("K", "n1", n1);
    current_statement__ = 297;
    validate_non_negative_index("K", "n2", n2);
    Eigen::Matrix<local_scalar_t__, -1, -1> K;
    K = Eigen::Matrix<local_scalar_t__, -1, -1>(n1, n2);
    stan::math::fill(K, DUMMY_VAR__);
    
    current_statement__ = 306;
    if (logical_eq(kernel_type, 1)) {
      current_statement__ = 304;
      assign(K, nil_index_list(), STAN_kernel_cat(x1, x2, pstream__),
        "assigning variable K");
    } else {
      current_statement__ = 303;
      if (logical_eq(kernel_type, 2)) {
        current_statement__ = 301;
        assign(K, nil_index_list(), STAN_kernel_bin(x1, x2, pstream__),
          "assigning variable K");
      } else {
        current_statement__ = 299;
        assign(K, nil_index_list(),
          STAN_kernel_zerosum(x1, x2, ncat, pstream__),
          "assigning variable K");
      }
    }
    current_statement__ = 307;
    return K;
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_kernel_const_functor__ {
Eigen::Matrix<double, -1, -1>
operator()(const std::vector<int>& x1, const std::vector<int>& x2,
           const int& kernel_type, const int& ncat, std::ostream* pstream__)  const 
{
return STAN_kernel_const(x1, x2, kernel_type, ncat, pstream__);
}
};
std::vector<Eigen::Matrix<double, -1, -1>>
STAN_kernel_const_all(const int& n1, const int& n2,
                      const std::vector<std::vector<int>>& x1,
                      const std::vector<std::vector<int>>& x2,
                      const std::vector<std::vector<int>>& x1_mask,
                      const std::vector<std::vector<int>>& x2_mask,
                      const std::vector<int>& num_levels,
                      const std::vector<std::vector<int>>& components,
                      std::ostream* pstream__) {
  using local_scalar_t__ = double;
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    int num_comps;
    num_comps = std::numeric_limits<int>::min();
    
    current_statement__ = 309;
    num_comps = stan::math::size(components);
    current_statement__ = 310;
    validate_non_negative_index("K_const", "num_comps", num_comps);
    current_statement__ = 311;
    validate_non_negative_index("K_const", "n1", n1);
    current_statement__ = 312;
    validate_non_negative_index("K_const", "n2", n2);
    std::vector<Eigen::Matrix<local_scalar_t__, -1, -1>> K_const;
    K_const = std::vector<Eigen::Matrix<local_scalar_t__, -1, -1>>(num_comps, Eigen::Matrix<local_scalar_t__, -1, -1>(n1, n2));
    stan::math::fill(K_const, DUMMY_VAR__);
    
    current_statement__ = 333;
    for (int j = 1; j <= num_comps; ++j) {
      current_statement__ = 314;
      validate_non_negative_index("K", "n1", n1);
      current_statement__ = 315;
      validate_non_negative_index("K", "n2", n2);
      Eigen::Matrix<local_scalar_t__, -1, -1> K;
      K = Eigen::Matrix<local_scalar_t__, -1, -1>(n1, n2);
      stan::math::fill(K, DUMMY_VAR__);
      
      std::vector<int> opts;
      opts = std::vector<int>(9, std::numeric_limits<int>::min());
      
      current_statement__ = 317;
      assign(opts, nil_index_list(), components[(j - 1)],
        "assigning variable opts");
      int ctype;
      ctype = std::numeric_limits<int>::min();
      
      current_statement__ = 318;
      ctype = opts[(1 - 1)];
      int ktype;
      ktype = std::numeric_limits<int>::min();
      
      current_statement__ = 319;
      ktype = opts[(2 - 1)];
      int idx_cat;
      idx_cat = std::numeric_limits<int>::min();
      
      current_statement__ = 320;
      idx_cat = opts[(8 - 1)];
      int idx_cont;
      idx_cont = std::numeric_limits<int>::min();
      
      current_statement__ = 321;
      idx_cont = opts[(9 - 1)];
      current_statement__ = 326;
      if (logical_neq(idx_cont, 0)) {
        current_statement__ = 324;
        assign(K, nil_index_list(),
          STAN_kernel_const(x1_mask[(idx_cont - 1)], x2_mask[(idx_cont - 1)],
            2, 0, pstream__), "assigning variable K");
      } else {
        current_statement__ = 322;
        assign(K, nil_index_list(), rep_matrix(1, n1, n2),
          "assigning variable K");
      }
      current_statement__ = 330;
      if ((primitive_value(logical_eq(ctype, 0)) || primitive_value(
          logical_eq(ctype, 2)))) {
        int M;
        M = std::numeric_limits<int>::min();
        
        current_statement__ = 327;
        M = num_levels[(idx_cat - 1)];
        current_statement__ = 328;
        assign(K, nil_index_list(),
          elt_multiply(stan::model::deep_copy(K),
            STAN_kernel_const(x1[(idx_cat - 1)], x2[(idx_cat - 1)], ktype,
              M, pstream__)), "assigning variable K");
      } 
      current_statement__ = 331;
      assign(K_const, cons_list(index_uni(j), nil_index_list()), K,
        "assigning variable K_const");}
    current_statement__ = 334;
    return K_const;
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_kernel_const_all_functor__ {
std::vector<Eigen::Matrix<double, -1, -1>>
operator()(const int& n1, const int& n2,
           const std::vector<std::vector<int>>& x1,
           const std::vector<std::vector<int>>& x2,
           const std::vector<std::vector<int>>& x1_mask,
           const std::vector<std::vector<int>>& x2_mask,
           const std::vector<int>& num_levels,
           const std::vector<std::vector<int>>& components,
           std::ostream* pstream__)  const 
{
return STAN_kernel_const_all(n1, n2, x1, x2, x1_mask, x2_mask, num_levels,
         components, pstream__);
}
};
template <typename T0__, typename T1__, typename T2__, typename T3__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>, stan::value_type_t<T1__>,
T2__,
T3__>, -1, -1>
STAN_kernel_eq(const T0__& x1_arg__, const T1__& x2_arg__, const T2__& alpha,
               const T3__& ell, std::ostream* pstream__) {
  using local_scalar_t__ = stan::promote_args_t<stan::value_type_t<T0__>,
          stan::value_type_t<T1__>,
          T2__,
          T3__>;
  const auto& x1 = to_ref(x1_arg__);
  const auto& x2 = to_ref(x2_arg__);
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    current_statement__ = 336;
    return gp_exp_quad_cov(to_array_1d(x1), to_array_1d(x2), alpha, ell);
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_kernel_eq_functor__ {
template <typename T0__, typename T1__, typename T2__, typename T3__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>, stan::value_type_t<T1__>,
T2__,
T3__>, -1, -1>
operator()(const T0__& x1, const T1__& x2, const T2__& alpha,
           const T3__& ell, std::ostream* pstream__)  const 
{
return STAN_kernel_eq(x1, x2, alpha, ell, pstream__);
}
};
template <typename T0__, typename T1__, typename T2__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>, stan::value_type_t<T1__>,
T2__>, -1, -1>
STAN_kernel_varmask(const T0__& x1_arg__, const T1__& x2_arg__,
                    const T2__& steepness,
                    const std::vector<double>& vm_params,
                    std::ostream* pstream__) {
  using local_scalar_t__ = stan::promote_args_t<stan::value_type_t<T0__>,
          stan::value_type_t<T1__>,
          T2__>;
  const auto& x1 = to_ref(x1_arg__);
  const auto& x2 = to_ref(x2_arg__);
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    local_scalar_t__ a;
    a = DUMMY_VAR__;
    
    current_statement__ = 338;
    a = (steepness * vm_params[(2 - 1)]);
    local_scalar_t__ r;
    r = DUMMY_VAR__;
    
    current_statement__ = 339;
    r = (inv(a) * logit(vm_params[(1 - 1)]));
    current_statement__ = 340;
    return multiply(
             to_matrix(
               to_matrix(STAN_var_mask(subtract(x1, r), a, pstream__))),
             transpose(
               to_matrix(STAN_var_mask(subtract(x2, r), a, pstream__))));
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_kernel_varmask_functor__ {
template <typename T0__, typename T1__, typename T2__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>, stan::value_type_t<T1__>,
T2__>, -1, -1>
operator()(const T0__& x1, const T1__& x2, const T2__& steepness,
           const std::vector<double>& vm_params, std::ostream* pstream__)  const 
{
return STAN_kernel_varmask(x1, x2, steepness, vm_params, pstream__);
}
};
template <typename T0__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>>, -1, -1>
STAN_kernel_beta(const T0__& beta_arg__, const std::vector<int>& idx1_expand,
                 const std::vector<int>& idx2_expand, std::ostream* pstream__) {
  using local_scalar_t__ = stan::promote_args_t<stan::value_type_t<T0__>>;
  const auto& beta = to_ref(beta_arg__);
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    current_statement__ = 342;
    return multiply(
             to_matrix(
               STAN_expand(stan::math::sqrt(beta), idx1_expand, pstream__)),
             transpose(
               to_matrix(
                 STAN_expand(stan::math::sqrt(beta), idx2_expand, pstream__))));
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_kernel_beta_functor__ {
template <typename T0__>
Eigen::Matrix<stan::promote_args_t<stan::value_type_t<T0__>>, -1, -1>
operator()(const T0__& beta, const std::vector<int>& idx1_expand,
           const std::vector<int>& idx2_expand, std::ostream* pstream__)  const 
{
return STAN_kernel_beta(beta, idx1_expand, idx2_expand, pstream__);
}
};
template <typename T8__, typename T9__, typename T10__, typename T11__,
typename T12__>
std::vector<Eigen::Matrix<stan::promote_args_t<T8__, T9__, T10__, T11__,
T12__>, -1, -1>>
STAN_kernel_all(const int& n1, const int& n2,
                const std::vector<Eigen::Matrix<double, -1, -1>>& K_const,
                const std::vector<std::vector<int>>& components,
                const std::vector<Eigen::Matrix<double, -1, 1>>& x1,
                const std::vector<Eigen::Matrix<double, -1, 1>>& x2,
                const std::vector<Eigen::Matrix<double, -1, 1>>& x1_unnorm,
                const std::vector<Eigen::Matrix<double, -1, 1>>& x2_unnorm,
                const std::vector<T8__>& alpha, const std::vector<T9__>& ell,
                const std::vector<T10__>& wrp,
                const std::vector<Eigen::Matrix<T11__, -1, 1>>& beta,
                const std::vector<Eigen::Matrix<T12__, -1, 1>>& teff,
                const std::vector<double>& vm_params,
                const std::vector<int>& idx1_expand,
                const std::vector<int>& idx2_expand,
                const std::vector<Eigen::Matrix<double, -1, 1>>& teff_zero,
                std::ostream* pstream__) {
  using local_scalar_t__ = stan::promote_args_t<T8__,
          T9__,
          T10__,
          T11__,
          T12__>;
  const static bool propto__ = true;
  (void) propto__;
  local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
  (void) DUMMY_VAR__;  // suppress unused var warning
  
  try {
    int idx_ell;
    idx_ell = std::numeric_limits<int>::min();
    
    current_statement__ = 344;
    idx_ell = 0;
    int idx_wrp;
    idx_wrp = std::numeric_limits<int>::min();
    
    current_statement__ = 345;
    idx_wrp = 0;
    int idx_alpha;
    idx_alpha = std::numeric_limits<int>::min();
    
    current_statement__ = 346;
    idx_alpha = 0;
    int num_comps;
    num_comps = std::numeric_limits<int>::min();
    
    current_statement__ = 347;
    num_comps = stan::math::size(components);
    current_statement__ = 348;
    validate_non_negative_index("KX", "num_comps", num_comps);
    current_statement__ = 349;
    validate_non_negative_index("KX", "n1", n1);
    current_statement__ = 350;
    validate_non_negative_index("KX", "n2", n2);
    std::vector<Eigen::Matrix<local_scalar_t__, -1, -1>> KX;
    KX = std::vector<Eigen::Matrix<local_scalar_t__, -1, -1>>(num_comps, Eigen::Matrix<local_scalar_t__, -1, -1>(n1, n2));
    stan::math::fill(KX, DUMMY_VAR__);
    
    current_statement__ = 401;
    for (int j = 1; j <= num_comps; ++j) {
      current_statement__ = 352;
      validate_non_negative_index("K", "n1", n1);
      current_statement__ = 353;
      validate_non_negative_index("K", "n2", n2);
      Eigen::Matrix<local_scalar_t__, -1, -1> K;
      K = Eigen::Matrix<local_scalar_t__, -1, -1>(n1, n2);
      stan::math::fill(K, DUMMY_VAR__);
      
      current_statement__ = 354;
      assign(K, nil_index_list(), K_const[(j - 1)], "assigning variable K");
      current_statement__ = 355;
      validate_non_negative_index("X1", "n1", n1);
      Eigen::Matrix<local_scalar_t__, -1, 1> X1;
      X1 = Eigen::Matrix<local_scalar_t__, -1, 1>(n1);
      stan::math::fill(X1, DUMMY_VAR__);
      
      current_statement__ = 357;
      validate_non_negative_index("X2", "n2", n2);
      Eigen::Matrix<local_scalar_t__, -1, 1> X2;
      X2 = Eigen::Matrix<local_scalar_t__, -1, 1>(n2);
      stan::math::fill(X2, DUMMY_VAR__);
      
      std::vector<int> opts;
      opts = std::vector<int>(9, std::numeric_limits<int>::min());
      
      current_statement__ = 359;
      assign(opts, nil_index_list(), components[(j - 1)],
        "assigning variable opts");
      int ctype;
      ctype = std::numeric_limits<int>::min();
      
      current_statement__ = 360;
      ctype = opts[(1 - 1)];
      int idx_cont;
      idx_cont = std::numeric_limits<int>::min();
      
      current_statement__ = 361;
      idx_cont = opts[(9 - 1)];
      int is_heter;
      is_heter = std::numeric_limits<int>::min();
      
      current_statement__ = 362;
      is_heter = opts[(4 - 1)];
      int is_warped;
      is_warped = std::numeric_limits<int>::min();
      
      current_statement__ = 363;
      is_warped = opts[(5 - 1)];
      int is_var_masked;
      is_var_masked = std::numeric_limits<int>::min();
      
      current_statement__ = 364;
      is_var_masked = opts[(6 - 1)];
      int is_uncrt;
      is_uncrt = std::numeric_limits<int>::min();
      
      current_statement__ = 365;
      is_uncrt = opts[(7 - 1)];
      current_statement__ = 374;
      if (logical_neq(ctype, 0)) {
        current_statement__ = 372;
        if (is_warped) {
          current_statement__ = 369;
          assign(X1, nil_index_list(), x1_unnorm[(idx_cont - 1)],
            "assigning variable X1");
          current_statement__ = 370;
          assign(X2, nil_index_list(), x2_unnorm[(idx_cont - 1)],
            "assigning variable X2");
        } else {
          current_statement__ = 366;
          assign(X1, nil_index_list(), x1[(idx_cont - 1)],
            "assigning variable X1");
          current_statement__ = 367;
          assign(X2, nil_index_list(), x2[(idx_cont - 1)],
            "assigning variable X2");
        }
      } 
      current_statement__ = 388;
      if (is_warped) {
        local_scalar_t__ s;
        s = DUMMY_VAR__;
        
        current_statement__ = 376;
        idx_wrp = (idx_wrp + 1);
        current_statement__ = 380;
        if (is_uncrt) {
          current_statement__ = 377;
          assign(X1, nil_index_list(),
            STAN_edit_x_cont(stan::model::deep_copy(X1), idx1_expand,
              teff_zero[(1 - 1)], teff[(1 - 1)], pstream__),
            "assigning variable X1");
          current_statement__ = 378;
          assign(X2, nil_index_list(),
            STAN_edit_x_cont(stan::model::deep_copy(X2), idx2_expand,
              teff_zero[(1 - 1)], teff[(1 - 1)], pstream__),
            "assigning variable X2");
        } 
        current_statement__ = 381;
        s = wrp[(idx_wrp - 1)];
        current_statement__ = 384;
        if (is_var_masked) {
          current_statement__ = 382;
          assign(K, nil_index_list(),
            elt_multiply(stan::model::deep_copy(K),
              STAN_kernel_varmask(X1, X2, s, vm_params, pstream__)),
            "assigning variable K");
        } 
        current_statement__ = 385;
        assign(X1, nil_index_list(),
          STAN_warp_input(stan::model::deep_copy(X1), s, pstream__),
          "assigning variable X1");
        current_statement__ = 386;
        assign(X2, nil_index_list(),
          STAN_warp_input(stan::model::deep_copy(X2), s, pstream__),
          "assigning variable X2");
      } 
      current_statement__ = 389;
      idx_alpha = (idx_alpha + 1);
      current_statement__ = 395;
      if (logical_neq(ctype, 0)) {
        current_statement__ = 392;
        idx_ell = (idx_ell + 1);
        current_statement__ = 393;
        assign(K, nil_index_list(),
          elt_multiply(stan::model::deep_copy(K),
            STAN_kernel_eq(X1, X2, alpha[(idx_alpha - 1)],
              ell[(idx_ell - 1)], pstream__)), "assigning variable K");
      } else {
        current_statement__ = 390;
        assign(K, nil_index_list(),
          multiply(square(alpha[(idx_alpha - 1)]), stan::model::deep_copy(K)),
          "assigning variable K");
      }
      current_statement__ = 398;
      if (is_heter) {
        current_statement__ = 396;
        assign(K, nil_index_list(),
          elt_multiply(stan::model::deep_copy(K),
            STAN_kernel_beta(beta[(1 - 1)], idx1_expand,
              idx2_expand, pstream__)), "assigning variable K");
      } 
      current_statement__ = 399;
      assign(KX, cons_list(index_uni(j), nil_index_list()), K,
        "assigning variable KX");}
    current_statement__ = 402;
    return KX;
  } catch (const std::exception& e) {
    stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
  }
  
}
struct STAN_kernel_all_functor__ {
template <typename T8__, typename T9__, typename T10__, typename T11__,
typename T12__>
std::vector<Eigen::Matrix<stan::promote_args_t<T8__, T9__, T10__, T11__,
T12__>, -1, -1>>
operator()(const int& n1, const int& n2,
           const std::vector<Eigen::Matrix<double, -1, -1>>& K_const,
           const std::vector<std::vector<int>>& components,
           const std::vector<Eigen::Matrix<double, -1, 1>>& x1,
           const std::vector<Eigen::Matrix<double, -1, 1>>& x2,
           const std::vector<Eigen::Matrix<double, -1, 1>>& x1_unnorm,
           const std::vector<Eigen::Matrix<double, -1, 1>>& x2_unnorm,
           const std::vector<T8__>& alpha, const std::vector<T9__>& ell,
           const std::vector<T10__>& wrp,
           const std::vector<Eigen::Matrix<T11__, -1, 1>>& beta,
           const std::vector<Eigen::Matrix<T12__, -1, 1>>& teff,
           const std::vector<double>& vm_params,
           const std::vector<int>& idx1_expand,
           const std::vector<int>& idx2_expand,
           const std::vector<Eigen::Matrix<double, -1, 1>>& teff_zero,
           std::ostream* pstream__)  const 
{
return STAN_kernel_all(n1, n2, K_const, components, x1, x2, x1_unnorm,
         x2_unnorm, alpha, ell, wrp, beta, teff, vm_params, idx1_expand,
         idx2_expand, teff_zero, pstream__);
}
};
#include <stan_meta_header.hpp>
class model_lgp_latent final : public model_base_crtp<model_lgp_latent> {
private:
  int is_verbose;
  int is_likelihood_skipped;
  int num_obs;
  int num_cov_cont;
  int num_cov_cat;
  int num_comps;
  int num_ell;
  int num_ns;
  int num_heter;
  int num_uncrt;
  int num_bt;
  std::vector<std::vector<int>> components;
  std::vector<Eigen::Matrix<double, -1, 1>> teff_zero;
  std::vector<Eigen::Matrix<double, -1, 1>> teff_lb;
  std::vector<Eigen::Matrix<double, -1, 1>> teff_ub;
  std::vector<int> x_cat_num_levels;
  double delta;
  std::vector<double> vm_params;
  std::vector<Eigen::Matrix<double, -1, 1>> x_cont;
  std::vector<Eigen::Matrix<double, -1, 1>> x_cont_unnorm;
  std::vector<std::vector<int>> x_cont_mask;
  std::vector<std::vector<int>> x_cat;
  std::vector<int> idx_expand;
  std::vector<std::vector<int>> prior_alpha;
  std::vector<std::vector<int>> prior_ell;
  std::vector<std::vector<int>> prior_wrp;
  std::vector<std::vector<int>> prior_teff;
  std::vector<std::vector<double>> hyper_alpha;
  std::vector<std::vector<double>> hyper_ell;
  std::vector<std::vector<double>> hyper_wrp;
  std::vector<std::vector<double>> hyper_teff;
  std::vector<std::vector<double>> hyper_beta;
  int obs_model;
  std::vector<std::vector<int>> y_int;
  std::vector<std::vector<double>> y_real;
  std::vector<std::vector<int>> y_num_trials;
  std::vector<std::vector<int>> prior_sigma;
  std::vector<std::vector<int>> prior_phi;
  std::vector<std::vector<double>> hyper_sigma;
  std::vector<std::vector<double>> hyper_phi;
  std::vector<std::vector<double>> hyper_gamma;
  Eigen::Matrix<double, -1, 1> c_hat;
  std::vector<Eigen::Matrix<double, -1, -1>> K_const;
  Eigen::Matrix<double, -1, 1> delta_vec;
  int beta_1dim__;
  int teff_raw_1dim__;
  int sigma_1dim__;
  int phi_1dim__;
  int gamma_1dim__;
  int teff_1dim__;
 
public:
  ~model_lgp_latent() { }
  
  inline std::string model_name() const final { return "model_lgp_latent"; }
  inline std::vector<std::string> model_compile_info() const noexcept {
    return std::vector<std::string>{"stanc_version = stanc3 v2.26.1-4-gd72b68b7-dirty", "stancflags = "};
  }
  
  
  model_lgp_latent(stan::io::var_context& context__,
                   unsigned int random_seed__ = 0,
                   std::ostream* pstream__ = nullptr) : model_base_crtp(0) {
    using local_scalar_t__ = double ;
    boost::ecuyer1988 base_rng__ = 
        stan::services::util::create_rng(random_seed__, 0);
    (void) base_rng__;  // suppress unused var warning
    static const char* function__ = "model_lgp_latent_namespace::model_lgp_latent";
    (void) function__;  // suppress unused var warning
    local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
    (void) DUMMY_VAR__;  // suppress unused var warning
    
    try {
      int pos__;
      pos__ = std::numeric_limits<int>::min();
      
      pos__ = 1;
      current_statement__ = 94;
      context__.validate_dims("data initialization","is_verbose","int",
          context__.to_vec());
      is_verbose = std::numeric_limits<int>::min();
      
      current_statement__ = 94;
      is_verbose = context__.vals_i("is_verbose")[(1 - 1)];
      current_statement__ = 94;
      current_statement__ = 94;
      check_greater_or_equal(function__, "is_verbose", is_verbose, 0);
      current_statement__ = 94;
      current_statement__ = 94;
      check_less_or_equal(function__, "is_verbose", is_verbose, 1);
      current_statement__ = 95;
      context__.validate_dims("data initialization","is_likelihood_skipped",
          "int",context__.to_vec());
      is_likelihood_skipped = std::numeric_limits<int>::min();
      
      current_statement__ = 95;
      is_likelihood_skipped = context__.vals_i("is_likelihood_skipped")[
          (1 - 1)];
      current_statement__ = 95;
      current_statement__ = 95;
      check_greater_or_equal(function__, "is_likelihood_skipped",
                             is_likelihood_skipped, 0);
      current_statement__ = 95;
      current_statement__ = 95;
      check_less_or_equal(function__, "is_likelihood_skipped",
                          is_likelihood_skipped, 1);
      current_statement__ = 96;
      context__.validate_dims("data initialization","num_obs","int",
          context__.to_vec());
      num_obs = std::numeric_limits<int>::min();
      
      current_statement__ = 96;
      num_obs = context__.vals_i("num_obs")[(1 - 1)];
      current_statement__ = 96;
      current_statement__ = 96;
      check_greater_or_equal(function__, "num_obs", num_obs, 0);
      current_statement__ = 97;
      context__.validate_dims("data initialization","num_cov_cont","int",
          context__.to_vec());
      num_cov_cont = std::numeric_limits<int>::min();
      
      current_statement__ = 97;
      num_cov_cont = context__.vals_i("num_cov_cont")[(1 - 1)];
      current_statement__ = 97;
      current_statement__ = 97;
      check_greater_or_equal(function__, "num_cov_cont", num_cov_cont, 0);
      current_statement__ = 98;
      context__.validate_dims("data initialization","num_cov_cat","int",
          context__.to_vec());
      num_cov_cat = std::numeric_limits<int>::min();
      
      current_statement__ = 98;
      num_cov_cat = context__.vals_i("num_cov_cat")[(1 - 1)];
      current_statement__ = 98;
      current_statement__ = 98;
      check_greater_or_equal(function__, "num_cov_cat", num_cov_cat, 0);
      current_statement__ = 99;
      context__.validate_dims("data initialization","num_comps","int",
          context__.to_vec());
      num_comps = std::numeric_limits<int>::min();
      
      current_statement__ = 99;
      num_comps = context__.vals_i("num_comps")[(1 - 1)];
      current_statement__ = 99;
      current_statement__ = 99;
      check_greater_or_equal(function__, "num_comps", num_comps, 1);
      current_statement__ = 100;
      context__.validate_dims("data initialization","num_ell","int",
          context__.to_vec());
      num_ell = std::numeric_limits<int>::min();
      
      current_statement__ = 100;
      num_ell = context__.vals_i("num_ell")[(1 - 1)];
      current_statement__ = 100;
      current_statement__ = 100;
      check_greater_or_equal(function__, "num_ell", num_ell, 0);
      current_statement__ = 101;
      context__.validate_dims("data initialization","num_ns","int",
          context__.to_vec());
      num_ns = std::numeric_limits<int>::min();
      
      current_statement__ = 101;
      num_ns = context__.vals_i("num_ns")[(1 - 1)];
      current_statement__ = 101;
      current_statement__ = 101;
      check_greater_or_equal(function__, "num_ns", num_ns, 0);
      current_statement__ = 102;
      context__.validate_dims("data initialization","num_heter","int",
          context__.to_vec());
      num_heter = std::numeric_limits<int>::min();
      
      current_statement__ = 102;
      num_heter = context__.vals_i("num_heter")[(1 - 1)];
      current_statement__ = 102;
      current_statement__ = 102;
      check_greater_or_equal(function__, "num_heter", num_heter, 0);
      current_statement__ = 103;
      context__.validate_dims("data initialization","num_uncrt","int",
          context__.to_vec());
      num_uncrt = std::numeric_limits<int>::min();
      
      current_statement__ = 103;
      num_uncrt = context__.vals_i("num_uncrt")[(1 - 1)];
      current_statement__ = 103;
      current_statement__ = 103;
      check_greater_or_equal(function__, "num_uncrt", num_uncrt, 0);
      current_statement__ = 104;
      context__.validate_dims("data initialization","num_bt","int",
          context__.to_vec());
      num_bt = std::numeric_limits<int>::min();
      
      current_statement__ = 104;
      num_bt = context__.vals_i("num_bt")[(1 - 1)];
      current_statement__ = 104;
      current_statement__ = 104;
      check_greater_or_equal(function__, "num_bt", num_bt, 0);
      current_statement__ = 105;
      validate_non_negative_index("components", "num_comps", num_comps);
      current_statement__ = 106;
      context__.validate_dims("data initialization","components","int",
          context__.to_vec(num_comps, 9));
      components = std::vector<std::vector<int>>(num_comps, std::vector<int>(9, std::numeric_limits<int>::min()));
      
      {
        std::vector<int> components_flat__;
        current_statement__ = 106;
        assign(components_flat__, nil_index_list(),
          context__.vals_i("components"),
          "assigning variable components_flat__");
        current_statement__ = 106;
        pos__ = 1;
        current_statement__ = 106;
        for (int sym1__ = 1; sym1__ <= 9; ++sym1__) {
          current_statement__ = 106;
          for (int sym2__ = 1; sym2__ <= num_comps; ++sym2__) {
            current_statement__ = 106;
            assign(components,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              components_flat__[(pos__ - 1)], "assigning variable components");
            current_statement__ = 106;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 106;
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        current_statement__ = 106;
        for (int sym2__ = 1; sym2__ <= 9; ++sym2__) {
          current_statement__ = 106;
          current_statement__ = 106;
          check_greater_or_equal(function__, "components[sym1__, sym2__]",
                                 components[(sym1__ - 1)][(sym2__ - 1)], 0);}
      }
      current_statement__ = 107;
      validate_non_negative_index("teff_zero", "num_uncrt > 0",
                                  logical_gt(num_uncrt, 0));
      current_statement__ = 108;
      validate_non_negative_index("teff_zero", "num_bt", num_bt);
      current_statement__ = 109;
      context__.validate_dims("data initialization","teff_zero","double",
          context__.to_vec(logical_gt(num_uncrt, 0), num_bt));
      teff_zero = std::vector<Eigen::Matrix<double, -1, 1>>(logical_gt(
                                                              num_uncrt, 0), Eigen::Matrix<double, -1, 1>(num_bt));
      stan::math::fill(teff_zero, std::numeric_limits<double>::quiet_NaN());
      
      {
        std::vector<local_scalar_t__> teff_zero_flat__;
        current_statement__ = 109;
        assign(teff_zero_flat__, nil_index_list(),
          context__.vals_r("teff_zero"),
          "assigning variable teff_zero_flat__");
        current_statement__ = 109;
        pos__ = 1;
        current_statement__ = 109;
        for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
          current_statement__ = 109;
          for (int sym2__ = 1; sym2__ <= logical_gt(num_uncrt, 0); ++sym2__) {
            current_statement__ = 109;
            assign(teff_zero,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              teff_zero_flat__[(pos__ - 1)], "assigning variable teff_zero");
            current_statement__ = 109;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 110;
      validate_non_negative_index("teff_lb", "num_uncrt > 0",
                                  logical_gt(num_uncrt, 0));
      current_statement__ = 111;
      validate_non_negative_index("teff_lb", "num_bt", num_bt);
      current_statement__ = 112;
      context__.validate_dims("data initialization","teff_lb","double",
          context__.to_vec(logical_gt(num_uncrt, 0), num_bt));
      teff_lb = std::vector<Eigen::Matrix<double, -1, 1>>(logical_gt(
                                                            num_uncrt, 0), Eigen::Matrix<double, -1, 1>(num_bt));
      stan::math::fill(teff_lb, std::numeric_limits<double>::quiet_NaN());
      
      {
        std::vector<local_scalar_t__> teff_lb_flat__;
        current_statement__ = 112;
        assign(teff_lb_flat__, nil_index_list(), context__.vals_r("teff_lb"),
          "assigning variable teff_lb_flat__");
        current_statement__ = 112;
        pos__ = 1;
        current_statement__ = 112;
        for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
          current_statement__ = 112;
          for (int sym2__ = 1; sym2__ <= logical_gt(num_uncrt, 0); ++sym2__) {
            current_statement__ = 112;
            assign(teff_lb,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              teff_lb_flat__[(pos__ - 1)], "assigning variable teff_lb");
            current_statement__ = 112;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 113;
      validate_non_negative_index("teff_ub", "num_uncrt > 0",
                                  logical_gt(num_uncrt, 0));
      current_statement__ = 114;
      validate_non_negative_index("teff_ub", "num_bt", num_bt);
      current_statement__ = 115;
      context__.validate_dims("data initialization","teff_ub","double",
          context__.to_vec(logical_gt(num_uncrt, 0), num_bt));
      teff_ub = std::vector<Eigen::Matrix<double, -1, 1>>(logical_gt(
                                                            num_uncrt, 0), Eigen::Matrix<double, -1, 1>(num_bt));
      stan::math::fill(teff_ub, std::numeric_limits<double>::quiet_NaN());
      
      {
        std::vector<local_scalar_t__> teff_ub_flat__;
        current_statement__ = 115;
        assign(teff_ub_flat__, nil_index_list(), context__.vals_r("teff_ub"),
          "assigning variable teff_ub_flat__");
        current_statement__ = 115;
        pos__ = 1;
        current_statement__ = 115;
        for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
          current_statement__ = 115;
          for (int sym2__ = 1; sym2__ <= logical_gt(num_uncrt, 0); ++sym2__) {
            current_statement__ = 115;
            assign(teff_ub,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              teff_ub_flat__[(pos__ - 1)], "assigning variable teff_ub");
            current_statement__ = 115;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 116;
      validate_non_negative_index("x_cat_num_levels", "num_cov_cat",
                                  num_cov_cat);
      current_statement__ = 117;
      context__.validate_dims("data initialization","x_cat_num_levels","int",
          context__.to_vec(num_cov_cat));
      x_cat_num_levels = std::vector<int>(num_cov_cat, std::numeric_limits<int>::min());
      
      current_statement__ = 117;
      assign(x_cat_num_levels, nil_index_list(),
        context__.vals_i("x_cat_num_levels"),
        "assigning variable x_cat_num_levels");
      current_statement__ = 117;
      for (int sym1__ = 1; sym1__ <= num_cov_cat; ++sym1__) {
        current_statement__ = 117;
        current_statement__ = 117;
        check_greater_or_equal(function__, "x_cat_num_levels[sym1__]",
                               x_cat_num_levels[(sym1__ - 1)], 0);}
      current_statement__ = 118;
      context__.validate_dims("data initialization","delta","double",
          context__.to_vec());
      delta = std::numeric_limits<double>::quiet_NaN();
      
      current_statement__ = 118;
      delta = context__.vals_r("delta")[(1 - 1)];
      current_statement__ = 119;
      context__.validate_dims("data initialization","vm_params","double",
          context__.to_vec(2));
      vm_params = std::vector<double>(2, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 119;
      assign(vm_params, nil_index_list(), context__.vals_r("vm_params"),
        "assigning variable vm_params");
      current_statement__ = 120;
      validate_non_negative_index("x_cont", "num_cov_cont", num_cov_cont);
      current_statement__ = 121;
      validate_non_negative_index("x_cont", "num_obs", num_obs);
      current_statement__ = 122;
      context__.validate_dims("data initialization","x_cont","double",
          context__.to_vec(num_cov_cont, num_obs));
      x_cont = std::vector<Eigen::Matrix<double, -1, 1>>(num_cov_cont, Eigen::Matrix<double, -1, 1>(num_obs));
      stan::math::fill(x_cont, std::numeric_limits<double>::quiet_NaN());
      
      {
        std::vector<local_scalar_t__> x_cont_flat__;
        current_statement__ = 122;
        assign(x_cont_flat__, nil_index_list(), context__.vals_r("x_cont"),
          "assigning variable x_cont_flat__");
        current_statement__ = 122;
        pos__ = 1;
        current_statement__ = 122;
        for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
          current_statement__ = 122;
          for (int sym2__ = 1; sym2__ <= num_cov_cont; ++sym2__) {
            current_statement__ = 122;
            assign(x_cont,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              x_cont_flat__[(pos__ - 1)], "assigning variable x_cont");
            current_statement__ = 122;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 123;
      validate_non_negative_index("x_cont_unnorm", "num_cov_cont",
                                  num_cov_cont);
      current_statement__ = 124;
      validate_non_negative_index("x_cont_unnorm", "num_obs", num_obs);
      current_statement__ = 125;
      context__.validate_dims("data initialization","x_cont_unnorm","double",
          context__.to_vec(num_cov_cont, num_obs));
      x_cont_unnorm = std::vector<Eigen::Matrix<double, -1, 1>>(num_cov_cont, Eigen::Matrix<double, -1, 1>(num_obs));
      stan::math::fill(x_cont_unnorm, std::numeric_limits<double>::quiet_NaN());
      
      {
        std::vector<local_scalar_t__> x_cont_unnorm_flat__;
        current_statement__ = 125;
        assign(x_cont_unnorm_flat__, nil_index_list(),
          context__.vals_r("x_cont_unnorm"),
          "assigning variable x_cont_unnorm_flat__");
        current_statement__ = 125;
        pos__ = 1;
        current_statement__ = 125;
        for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
          current_statement__ = 125;
          for (int sym2__ = 1; sym2__ <= num_cov_cont; ++sym2__) {
            current_statement__ = 125;
            assign(x_cont_unnorm,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              x_cont_unnorm_flat__[(pos__ - 1)],
              "assigning variable x_cont_unnorm");
            current_statement__ = 125;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 126;
      validate_non_negative_index("x_cont_mask", "num_cov_cont", num_cov_cont);
      current_statement__ = 127;
      validate_non_negative_index("x_cont_mask", "num_obs", num_obs);
      current_statement__ = 128;
      context__.validate_dims("data initialization","x_cont_mask","int",
          context__.to_vec(num_cov_cont, num_obs));
      x_cont_mask = std::vector<std::vector<int>>(num_cov_cont, std::vector<int>(num_obs, std::numeric_limits<int>::min()));
      
      {
        std::vector<int> x_cont_mask_flat__;
        current_statement__ = 128;
        assign(x_cont_mask_flat__, nil_index_list(),
          context__.vals_i("x_cont_mask"),
          "assigning variable x_cont_mask_flat__");
        current_statement__ = 128;
        pos__ = 1;
        current_statement__ = 128;
        for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
          current_statement__ = 128;
          for (int sym2__ = 1; sym2__ <= num_cov_cont; ++sym2__) {
            current_statement__ = 128;
            assign(x_cont_mask,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              x_cont_mask_flat__[(pos__ - 1)],
              "assigning variable x_cont_mask");
            current_statement__ = 128;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 129;
      validate_non_negative_index("x_cat", "num_cov_cat", num_cov_cat);
      current_statement__ = 130;
      validate_non_negative_index("x_cat", "num_obs", num_obs);
      current_statement__ = 131;
      context__.validate_dims("data initialization","x_cat","int",
          context__.to_vec(num_cov_cat, num_obs));
      x_cat = std::vector<std::vector<int>>(num_cov_cat, std::vector<int>(num_obs, std::numeric_limits<int>::min()));
      
      {
        std::vector<int> x_cat_flat__;
        current_statement__ = 131;
        assign(x_cat_flat__, nil_index_list(), context__.vals_i("x_cat"),
          "assigning variable x_cat_flat__");
        current_statement__ = 131;
        pos__ = 1;
        current_statement__ = 131;
        for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
          current_statement__ = 131;
          for (int sym2__ = 1; sym2__ <= num_cov_cat; ++sym2__) {
            current_statement__ = 131;
            assign(x_cat,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              x_cat_flat__[(pos__ - 1)], "assigning variable x_cat");
            current_statement__ = 131;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 132;
      validate_non_negative_index("idx_expand", "num_obs", num_obs);
      current_statement__ = 133;
      context__.validate_dims("data initialization","idx_expand","int",
          context__.to_vec(num_obs));
      idx_expand = std::vector<int>(num_obs, std::numeric_limits<int>::min());
      
      current_statement__ = 133;
      assign(idx_expand, nil_index_list(), context__.vals_i("idx_expand"),
        "assigning variable idx_expand");
      current_statement__ = 133;
      for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
        current_statement__ = 133;
        current_statement__ = 133;
        check_greater_or_equal(function__, "idx_expand[sym1__]",
                               idx_expand[(sym1__ - 1)], 1);}
      current_statement__ = 133;
      for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
        current_statement__ = 133;
        current_statement__ = 133;
        check_less_or_equal(function__, "idx_expand[sym1__]",
                            idx_expand[(sym1__ - 1)], (num_bt + 1));}
      current_statement__ = 134;
      validate_non_negative_index("prior_alpha", "num_comps", num_comps);
      current_statement__ = 135;
      context__.validate_dims("data initialization","prior_alpha","int",
          context__.to_vec(num_comps, 2));
      prior_alpha = std::vector<std::vector<int>>(num_comps, std::vector<int>(2, std::numeric_limits<int>::min()));
      
      {
        std::vector<int> prior_alpha_flat__;
        current_statement__ = 135;
        assign(prior_alpha_flat__, nil_index_list(),
          context__.vals_i("prior_alpha"),
          "assigning variable prior_alpha_flat__");
        current_statement__ = 135;
        pos__ = 1;
        current_statement__ = 135;
        for (int sym1__ = 1; sym1__ <= 2; ++sym1__) {
          current_statement__ = 135;
          for (int sym2__ = 1; sym2__ <= num_comps; ++sym2__) {
            current_statement__ = 135;
            assign(prior_alpha,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              prior_alpha_flat__[(pos__ - 1)],
              "assigning variable prior_alpha");
            current_statement__ = 135;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 135;
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        current_statement__ = 135;
        for (int sym2__ = 1; sym2__ <= 2; ++sym2__) {
          current_statement__ = 135;
          current_statement__ = 135;
          check_greater_or_equal(function__, "prior_alpha[sym1__, sym2__]",
                                 prior_alpha[(sym1__ - 1)][(sym2__ - 1)], 0);
        }}
      current_statement__ = 136;
      validate_non_negative_index("prior_ell", "num_ell", num_ell);
      current_statement__ = 137;
      context__.validate_dims("data initialization","prior_ell","int",
          context__.to_vec(num_ell, 2));
      prior_ell = std::vector<std::vector<int>>(num_ell, std::vector<int>(2, std::numeric_limits<int>::min()));
      
      {
        std::vector<int> prior_ell_flat__;
        current_statement__ = 137;
        assign(prior_ell_flat__, nil_index_list(),
          context__.vals_i("prior_ell"),
          "assigning variable prior_ell_flat__");
        current_statement__ = 137;
        pos__ = 1;
        current_statement__ = 137;
        for (int sym1__ = 1; sym1__ <= 2; ++sym1__) {
          current_statement__ = 137;
          for (int sym2__ = 1; sym2__ <= num_ell; ++sym2__) {
            current_statement__ = 137;
            assign(prior_ell,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              prior_ell_flat__[(pos__ - 1)], "assigning variable prior_ell");
            current_statement__ = 137;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 137;
      for (int sym1__ = 1; sym1__ <= num_ell; ++sym1__) {
        current_statement__ = 137;
        for (int sym2__ = 1; sym2__ <= 2; ++sym2__) {
          current_statement__ = 137;
          current_statement__ = 137;
          check_greater_or_equal(function__, "prior_ell[sym1__, sym2__]",
                                 prior_ell[(sym1__ - 1)][(sym2__ - 1)], 0);}}
      current_statement__ = 138;
      validate_non_negative_index("prior_wrp", "num_ns", num_ns);
      current_statement__ = 139;
      context__.validate_dims("data initialization","prior_wrp","int",
          context__.to_vec(num_ns, 2));
      prior_wrp = std::vector<std::vector<int>>(num_ns, std::vector<int>(2, std::numeric_limits<int>::min()));
      
      {
        std::vector<int> prior_wrp_flat__;
        current_statement__ = 139;
        assign(prior_wrp_flat__, nil_index_list(),
          context__.vals_i("prior_wrp"),
          "assigning variable prior_wrp_flat__");
        current_statement__ = 139;
        pos__ = 1;
        current_statement__ = 139;
        for (int sym1__ = 1; sym1__ <= 2; ++sym1__) {
          current_statement__ = 139;
          for (int sym2__ = 1; sym2__ <= num_ns; ++sym2__) {
            current_statement__ = 139;
            assign(prior_wrp,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              prior_wrp_flat__[(pos__ - 1)], "assigning variable prior_wrp");
            current_statement__ = 139;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 139;
      for (int sym1__ = 1; sym1__ <= num_ns; ++sym1__) {
        current_statement__ = 139;
        for (int sym2__ = 1; sym2__ <= 2; ++sym2__) {
          current_statement__ = 139;
          current_statement__ = 139;
          check_greater_or_equal(function__, "prior_wrp[sym1__, sym2__]",
                                 prior_wrp[(sym1__ - 1)][(sym2__ - 1)], 0);}}
      current_statement__ = 140;
      validate_non_negative_index("prior_teff", "num_uncrt > 0",
                                  logical_gt(num_uncrt, 0));
      current_statement__ = 141;
      context__.validate_dims("data initialization","prior_teff","int",
          context__.to_vec(logical_gt(num_uncrt, 0), 2));
      prior_teff = std::vector<std::vector<int>>(logical_gt(num_uncrt, 0), std::vector<int>(2, std::numeric_limits<int>::min()));
      
      {
        std::vector<int> prior_teff_flat__;
        current_statement__ = 141;
        assign(prior_teff_flat__, nil_index_list(),
          context__.vals_i("prior_teff"),
          "assigning variable prior_teff_flat__");
        current_statement__ = 141;
        pos__ = 1;
        current_statement__ = 141;
        for (int sym1__ = 1; sym1__ <= 2; ++sym1__) {
          current_statement__ = 141;
          for (int sym2__ = 1; sym2__ <= logical_gt(num_uncrt, 0); ++sym2__) {
            current_statement__ = 141;
            assign(prior_teff,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              prior_teff_flat__[(pos__ - 1)], "assigning variable prior_teff");
            current_statement__ = 141;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 141;
      for (int sym1__ = 1; sym1__ <= logical_gt(num_uncrt, 0); ++sym1__) {
        current_statement__ = 141;
        for (int sym2__ = 1; sym2__ <= 2; ++sym2__) {
          current_statement__ = 141;
          current_statement__ = 141;
          check_greater_or_equal(function__, "prior_teff[sym1__, sym2__]",
                                 prior_teff[(sym1__ - 1)][(sym2__ - 1)], 0);}
      }
      current_statement__ = 142;
      validate_non_negative_index("hyper_alpha", "num_comps", num_comps);
      current_statement__ = 143;
      context__.validate_dims("data initialization","hyper_alpha","double",
          context__.to_vec(num_comps, 3));
      hyper_alpha = std::vector<std::vector<double>>(num_comps, std::vector<double>(3, std::numeric_limits<double>::quiet_NaN()));
      
      {
        std::vector<local_scalar_t__> hyper_alpha_flat__;
        current_statement__ = 143;
        assign(hyper_alpha_flat__, nil_index_list(),
          context__.vals_r("hyper_alpha"),
          "assigning variable hyper_alpha_flat__");
        current_statement__ = 143;
        pos__ = 1;
        current_statement__ = 143;
        for (int sym1__ = 1; sym1__ <= 3; ++sym1__) {
          current_statement__ = 143;
          for (int sym2__ = 1; sym2__ <= num_comps; ++sym2__) {
            current_statement__ = 143;
            assign(hyper_alpha,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              hyper_alpha_flat__[(pos__ - 1)],
              "assigning variable hyper_alpha");
            current_statement__ = 143;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 144;
      validate_non_negative_index("hyper_ell", "num_ell", num_ell);
      current_statement__ = 145;
      context__.validate_dims("data initialization","hyper_ell","double",
          context__.to_vec(num_ell, 3));
      hyper_ell = std::vector<std::vector<double>>(num_ell, std::vector<double>(3, std::numeric_limits<double>::quiet_NaN()));
      
      {
        std::vector<local_scalar_t__> hyper_ell_flat__;
        current_statement__ = 145;
        assign(hyper_ell_flat__, nil_index_list(),
          context__.vals_r("hyper_ell"),
          "assigning variable hyper_ell_flat__");
        current_statement__ = 145;
        pos__ = 1;
        current_statement__ = 145;
        for (int sym1__ = 1; sym1__ <= 3; ++sym1__) {
          current_statement__ = 145;
          for (int sym2__ = 1; sym2__ <= num_ell; ++sym2__) {
            current_statement__ = 145;
            assign(hyper_ell,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              hyper_ell_flat__[(pos__ - 1)], "assigning variable hyper_ell");
            current_statement__ = 145;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 146;
      validate_non_negative_index("hyper_wrp", "num_ns", num_ns);
      current_statement__ = 147;
      context__.validate_dims("data initialization","hyper_wrp","double",
          context__.to_vec(num_ns, 3));
      hyper_wrp = std::vector<std::vector<double>>(num_ns, std::vector<double>(3, std::numeric_limits<double>::quiet_NaN()));
      
      {
        std::vector<local_scalar_t__> hyper_wrp_flat__;
        current_statement__ = 147;
        assign(hyper_wrp_flat__, nil_index_list(),
          context__.vals_r("hyper_wrp"),
          "assigning variable hyper_wrp_flat__");
        current_statement__ = 147;
        pos__ = 1;
        current_statement__ = 147;
        for (int sym1__ = 1; sym1__ <= 3; ++sym1__) {
          current_statement__ = 147;
          for (int sym2__ = 1; sym2__ <= num_ns; ++sym2__) {
            current_statement__ = 147;
            assign(hyper_wrp,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              hyper_wrp_flat__[(pos__ - 1)], "assigning variable hyper_wrp");
            current_statement__ = 147;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 148;
      validate_non_negative_index("hyper_teff", "num_uncrt > 0",
                                  logical_gt(num_uncrt, 0));
      current_statement__ = 149;
      context__.validate_dims("data initialization","hyper_teff","double",
          context__.to_vec(logical_gt(num_uncrt, 0), 3));
      hyper_teff = std::vector<std::vector<double>>(logical_gt(num_uncrt, 0), std::vector<double>(3, std::numeric_limits<double>::quiet_NaN()));
      
      {
        std::vector<local_scalar_t__> hyper_teff_flat__;
        current_statement__ = 149;
        assign(hyper_teff_flat__, nil_index_list(),
          context__.vals_r("hyper_teff"),
          "assigning variable hyper_teff_flat__");
        current_statement__ = 149;
        pos__ = 1;
        current_statement__ = 149;
        for (int sym1__ = 1; sym1__ <= 3; ++sym1__) {
          current_statement__ = 149;
          for (int sym2__ = 1; sym2__ <= logical_gt(num_uncrt, 0); ++sym2__) {
            current_statement__ = 149;
            assign(hyper_teff,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              hyper_teff_flat__[(pos__ - 1)], "assigning variable hyper_teff");
            current_statement__ = 149;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 150;
      validate_non_negative_index("hyper_beta", "num_heter > 0",
                                  logical_gt(num_heter, 0));
      current_statement__ = 151;
      context__.validate_dims("data initialization","hyper_beta","double",
          context__.to_vec(logical_gt(num_heter, 0), 2));
      hyper_beta = std::vector<std::vector<double>>(logical_gt(num_heter, 0), std::vector<double>(2, std::numeric_limits<double>::quiet_NaN()));
      
      {
        std::vector<local_scalar_t__> hyper_beta_flat__;
        current_statement__ = 151;
        assign(hyper_beta_flat__, nil_index_list(),
          context__.vals_r("hyper_beta"),
          "assigning variable hyper_beta_flat__");
        current_statement__ = 151;
        pos__ = 1;
        current_statement__ = 151;
        for (int sym1__ = 1; sym1__ <= 2; ++sym1__) {
          current_statement__ = 151;
          for (int sym2__ = 1; sym2__ <= logical_gt(num_heter, 0); ++sym2__) {
            current_statement__ = 151;
            assign(hyper_beta,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              hyper_beta_flat__[(pos__ - 1)], "assigning variable hyper_beta");
            current_statement__ = 151;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 152;
      context__.validate_dims("data initialization","obs_model","int",
          context__.to_vec());
      obs_model = std::numeric_limits<int>::min();
      
      current_statement__ = 152;
      obs_model = context__.vals_i("obs_model")[(1 - 1)];
      current_statement__ = 152;
      current_statement__ = 152;
      check_greater_or_equal(function__, "obs_model", obs_model, 1);
      current_statement__ = 152;
      current_statement__ = 152;
      check_less_or_equal(function__, "obs_model", obs_model, 5);
      current_statement__ = 153;
      validate_non_negative_index("y_int", "obs_model > 1",
                                  logical_gt(obs_model, 1));
      current_statement__ = 154;
      validate_non_negative_index("y_int", "num_obs", num_obs);
      current_statement__ = 155;
      context__.validate_dims("data initialization","y_int","int",
          context__.to_vec(logical_gt(obs_model, 1), num_obs));
      y_int = std::vector<std::vector<int>>(logical_gt(obs_model, 1), std::vector<int>(num_obs, std::numeric_limits<int>::min()));
      
      {
        std::vector<int> y_int_flat__;
        current_statement__ = 155;
        assign(y_int_flat__, nil_index_list(), context__.vals_i("y_int"),
          "assigning variable y_int_flat__");
        current_statement__ = 155;
        pos__ = 1;
        current_statement__ = 155;
        for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
          current_statement__ = 155;
          for (int sym2__ = 1; sym2__ <= logical_gt(obs_model, 1); ++sym2__) {
            current_statement__ = 155;
            assign(y_int,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              y_int_flat__[(pos__ - 1)], "assigning variable y_int");
            current_statement__ = 155;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 155;
      for (int sym1__ = 1; sym1__ <= logical_gt(obs_model, 1); ++sym1__) {
        current_statement__ = 155;
        for (int sym2__ = 1; sym2__ <= num_obs; ++sym2__) {
          current_statement__ = 155;
          current_statement__ = 155;
          check_greater_or_equal(function__, "y_int[sym1__, sym2__]",
                                 y_int[(sym1__ - 1)][(sym2__ - 1)], 0);}}
      current_statement__ = 156;
      validate_non_negative_index("y_real", "obs_model == 1",
                                  logical_eq(obs_model, 1));
      current_statement__ = 157;
      validate_non_negative_index("y_real", "num_obs", num_obs);
      current_statement__ = 158;
      context__.validate_dims("data initialization","y_real","double",
          context__.to_vec(logical_eq(obs_model, 1), num_obs));
      y_real = std::vector<std::vector<double>>(logical_eq(obs_model, 1), std::vector<double>(num_obs, std::numeric_limits<double>::quiet_NaN()));
      
      {
        std::vector<local_scalar_t__> y_real_flat__;
        current_statement__ = 158;
        assign(y_real_flat__, nil_index_list(), context__.vals_r("y_real"),
          "assigning variable y_real_flat__");
        current_statement__ = 158;
        pos__ = 1;
        current_statement__ = 158;
        for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
          current_statement__ = 158;
          for (int sym2__ = 1; sym2__ <= logical_eq(obs_model, 1); ++sym2__) {
            current_statement__ = 158;
            assign(y_real,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              y_real_flat__[(pos__ - 1)], "assigning variable y_real");
            current_statement__ = 158;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 159;
      validate_non_negative_index("y_num_trials", "obs_model > 3",
                                  logical_gt(obs_model, 3));
      current_statement__ = 160;
      validate_non_negative_index("y_num_trials", "num_obs", num_obs);
      current_statement__ = 161;
      context__.validate_dims("data initialization","y_num_trials","int",
          context__.to_vec(logical_gt(obs_model, 3), num_obs));
      y_num_trials = std::vector<std::vector<int>>(logical_gt(obs_model, 3), std::vector<int>(num_obs, std::numeric_limits<int>::min()));
      
      {
        std::vector<int> y_num_trials_flat__;
        current_statement__ = 161;
        assign(y_num_trials_flat__, nil_index_list(),
          context__.vals_i("y_num_trials"),
          "assigning variable y_num_trials_flat__");
        current_statement__ = 161;
        pos__ = 1;
        current_statement__ = 161;
        for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
          current_statement__ = 161;
          for (int sym2__ = 1; sym2__ <= logical_gt(obs_model, 3); ++sym2__) {
            current_statement__ = 161;
            assign(y_num_trials,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              y_num_trials_flat__[(pos__ - 1)],
              "assigning variable y_num_trials");
            current_statement__ = 161;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 161;
      for (int sym1__ = 1; sym1__ <= logical_gt(obs_model, 3); ++sym1__) {
        current_statement__ = 161;
        for (int sym2__ = 1; sym2__ <= num_obs; ++sym2__) {
          current_statement__ = 161;
          current_statement__ = 161;
          check_greater_or_equal(function__, "y_num_trials[sym1__, sym2__]",
                                 y_num_trials[(sym1__ - 1)][(sym2__ - 1)], 1);
        }}
      current_statement__ = 162;
      validate_non_negative_index("prior_sigma", "obs_model == 1",
                                  logical_eq(obs_model, 1));
      current_statement__ = 163;
      context__.validate_dims("data initialization","prior_sigma","int",
          context__.to_vec(logical_eq(obs_model, 1), 2));
      prior_sigma = std::vector<std::vector<int>>(logical_eq(obs_model, 1), std::vector<int>(2, std::numeric_limits<int>::min()));
      
      {
        std::vector<int> prior_sigma_flat__;
        current_statement__ = 163;
        assign(prior_sigma_flat__, nil_index_list(),
          context__.vals_i("prior_sigma"),
          "assigning variable prior_sigma_flat__");
        current_statement__ = 163;
        pos__ = 1;
        current_statement__ = 163;
        for (int sym1__ = 1; sym1__ <= 2; ++sym1__) {
          current_statement__ = 163;
          for (int sym2__ = 1; sym2__ <= logical_eq(obs_model, 1); ++sym2__) {
            current_statement__ = 163;
            assign(prior_sigma,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              prior_sigma_flat__[(pos__ - 1)],
              "assigning variable prior_sigma");
            current_statement__ = 163;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 163;
      for (int sym1__ = 1; sym1__ <= logical_eq(obs_model, 1); ++sym1__) {
        current_statement__ = 163;
        for (int sym2__ = 1; sym2__ <= 2; ++sym2__) {
          current_statement__ = 163;
          current_statement__ = 163;
          check_greater_or_equal(function__, "prior_sigma[sym1__, sym2__]",
                                 prior_sigma[(sym1__ - 1)][(sym2__ - 1)], 0);
        }}
      current_statement__ = 164;
      validate_non_negative_index("prior_phi", "obs_model == 3",
                                  logical_eq(obs_model, 3));
      current_statement__ = 165;
      context__.validate_dims("data initialization","prior_phi","int",
          context__.to_vec(logical_eq(obs_model, 3), 2));
      prior_phi = std::vector<std::vector<int>>(logical_eq(obs_model, 3), std::vector<int>(2, std::numeric_limits<int>::min()));
      
      {
        std::vector<int> prior_phi_flat__;
        current_statement__ = 165;
        assign(prior_phi_flat__, nil_index_list(),
          context__.vals_i("prior_phi"),
          "assigning variable prior_phi_flat__");
        current_statement__ = 165;
        pos__ = 1;
        current_statement__ = 165;
        for (int sym1__ = 1; sym1__ <= 2; ++sym1__) {
          current_statement__ = 165;
          for (int sym2__ = 1; sym2__ <= logical_eq(obs_model, 3); ++sym2__) {
            current_statement__ = 165;
            assign(prior_phi,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              prior_phi_flat__[(pos__ - 1)], "assigning variable prior_phi");
            current_statement__ = 165;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 165;
      for (int sym1__ = 1; sym1__ <= logical_eq(obs_model, 3); ++sym1__) {
        current_statement__ = 165;
        for (int sym2__ = 1; sym2__ <= 2; ++sym2__) {
          current_statement__ = 165;
          current_statement__ = 165;
          check_greater_or_equal(function__, "prior_phi[sym1__, sym2__]",
                                 prior_phi[(sym1__ - 1)][(sym2__ - 1)], 0);}}
      current_statement__ = 166;
      validate_non_negative_index("hyper_sigma", "obs_model == 1",
                                  logical_eq(obs_model, 1));
      current_statement__ = 167;
      context__.validate_dims("data initialization","hyper_sigma","double",
          context__.to_vec(logical_eq(obs_model, 1), 3));
      hyper_sigma = std::vector<std::vector<double>>(logical_eq(obs_model, 1), std::vector<double>(3, std::numeric_limits<double>::quiet_NaN()));
      
      {
        std::vector<local_scalar_t__> hyper_sigma_flat__;
        current_statement__ = 167;
        assign(hyper_sigma_flat__, nil_index_list(),
          context__.vals_r("hyper_sigma"),
          "assigning variable hyper_sigma_flat__");
        current_statement__ = 167;
        pos__ = 1;
        current_statement__ = 167;
        for (int sym1__ = 1; sym1__ <= 3; ++sym1__) {
          current_statement__ = 167;
          for (int sym2__ = 1; sym2__ <= logical_eq(obs_model, 1); ++sym2__) {
            current_statement__ = 167;
            assign(hyper_sigma,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              hyper_sigma_flat__[(pos__ - 1)],
              "assigning variable hyper_sigma");
            current_statement__ = 167;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 168;
      validate_non_negative_index("hyper_phi", "obs_model == 3",
                                  logical_eq(obs_model, 3));
      current_statement__ = 169;
      context__.validate_dims("data initialization","hyper_phi","double",
          context__.to_vec(logical_eq(obs_model, 3), 3));
      hyper_phi = std::vector<std::vector<double>>(logical_eq(obs_model, 3), std::vector<double>(3, std::numeric_limits<double>::quiet_NaN()));
      
      {
        std::vector<local_scalar_t__> hyper_phi_flat__;
        current_statement__ = 169;
        assign(hyper_phi_flat__, nil_index_list(),
          context__.vals_r("hyper_phi"),
          "assigning variable hyper_phi_flat__");
        current_statement__ = 169;
        pos__ = 1;
        current_statement__ = 169;
        for (int sym1__ = 1; sym1__ <= 3; ++sym1__) {
          current_statement__ = 169;
          for (int sym2__ = 1; sym2__ <= logical_eq(obs_model, 3); ++sym2__) {
            current_statement__ = 169;
            assign(hyper_phi,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              hyper_phi_flat__[(pos__ - 1)], "assigning variable hyper_phi");
            current_statement__ = 169;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 170;
      validate_non_negative_index("hyper_gamma", "obs_model == 5",
                                  logical_eq(obs_model, 5));
      current_statement__ = 171;
      context__.validate_dims("data initialization","hyper_gamma","double",
          context__.to_vec(logical_eq(obs_model, 5), 2));
      hyper_gamma = std::vector<std::vector<double>>(logical_eq(obs_model, 5), std::vector<double>(2, std::numeric_limits<double>::quiet_NaN()));
      
      {
        std::vector<local_scalar_t__> hyper_gamma_flat__;
        current_statement__ = 171;
        assign(hyper_gamma_flat__, nil_index_list(),
          context__.vals_r("hyper_gamma"),
          "assigning variable hyper_gamma_flat__");
        current_statement__ = 171;
        pos__ = 1;
        current_statement__ = 171;
        for (int sym1__ = 1; sym1__ <= 2; ++sym1__) {
          current_statement__ = 171;
          for (int sym2__ = 1; sym2__ <= logical_eq(obs_model, 5); ++sym2__) {
            current_statement__ = 171;
            assign(hyper_gamma,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              hyper_gamma_flat__[(pos__ - 1)],
              "assigning variable hyper_gamma");
            current_statement__ = 171;
            pos__ = (pos__ + 1);}}
      }
      current_statement__ = 172;
      validate_non_negative_index("c_hat", "num_obs", num_obs);
      current_statement__ = 173;
      context__.validate_dims("data initialization","c_hat","double",
          context__.to_vec(num_obs));
      c_hat = Eigen::Matrix<double, -1, 1>(num_obs);
      stan::math::fill(c_hat, std::numeric_limits<double>::quiet_NaN());
      
      {
        std::vector<local_scalar_t__> c_hat_flat__;
        current_statement__ = 173;
        assign(c_hat_flat__, nil_index_list(), context__.vals_r("c_hat"),
          "assigning variable c_hat_flat__");
        current_statement__ = 173;
        pos__ = 1;
        current_statement__ = 173;
        for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
          current_statement__ = 173;
          assign(c_hat, cons_list(index_uni(sym1__), nil_index_list()),
            c_hat_flat__[(pos__ - 1)], "assigning variable c_hat");
          current_statement__ = 173;
          pos__ = (pos__ + 1);}
      }
      current_statement__ = 174;
      validate_non_negative_index("K_const", "num_comps", num_comps);
      current_statement__ = 175;
      validate_non_negative_index("K_const", "num_obs", num_obs);
      current_statement__ = 176;
      validate_non_negative_index("K_const", "num_obs", num_obs);
      current_statement__ = 177;
      K_const = std::vector<Eigen::Matrix<double, -1, -1>>(num_comps, Eigen::Matrix<double, -1, -1>(num_obs, num_obs));
      stan::math::fill(K_const, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 177;
      assign(K_const, nil_index_list(),
        STAN_kernel_const_all(num_obs, num_obs, x_cat, x_cat, x_cont_mask,
          x_cont_mask, x_cat_num_levels, components, pstream__),
        "assigning variable K_const");
      current_statement__ = 178;
      validate_non_negative_index("delta_vec", "num_obs", num_obs);
      current_statement__ = 179;
      delta_vec = Eigen::Matrix<double, -1, 1>(num_obs);
      stan::math::fill(delta_vec, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 179;
      assign(delta_vec, nil_index_list(), rep_vector(delta, num_obs),
        "assigning variable delta_vec");
      current_statement__ = 180;
      validate_non_negative_index("alpha", "num_comps", num_comps);
      current_statement__ = 181;
      validate_non_negative_index("ell", "num_ell", num_ell);
      current_statement__ = 182;
      validate_non_negative_index("wrp", "num_ns", num_ns);
      current_statement__ = 183;
      beta_1dim__ = std::numeric_limits<int>::min();
      
      current_statement__ = 183;
      beta_1dim__ = logical_gt(num_heter, 0);
      current_statement__ = 183;
      validate_non_negative_index("beta", "num_heter > 0", beta_1dim__);
      current_statement__ = 184;
      validate_non_negative_index("beta", "num_bt", num_bt);
      current_statement__ = 185;
      teff_raw_1dim__ = std::numeric_limits<int>::min();
      
      current_statement__ = 185;
      teff_raw_1dim__ = logical_gt(num_uncrt, 0);
      current_statement__ = 185;
      validate_non_negative_index("teff_raw", "num_uncrt > 0",
                                  teff_raw_1dim__);
      current_statement__ = 186;
      validate_non_negative_index("teff_raw", "num_bt", num_bt);
      current_statement__ = 187;
      sigma_1dim__ = std::numeric_limits<int>::min();
      
      current_statement__ = 187;
      sigma_1dim__ = logical_eq(obs_model, 1);
      current_statement__ = 187;
      validate_non_negative_index("sigma", "obs_model == 1", sigma_1dim__);
      current_statement__ = 188;
      phi_1dim__ = std::numeric_limits<int>::min();
      
      current_statement__ = 188;
      phi_1dim__ = logical_eq(obs_model, 3);
      current_statement__ = 188;
      validate_non_negative_index("phi", "obs_model == 3", phi_1dim__);
      current_statement__ = 189;
      gamma_1dim__ = std::numeric_limits<int>::min();
      
      current_statement__ = 189;
      gamma_1dim__ = logical_eq(obs_model, 5);
      current_statement__ = 189;
      validate_non_negative_index("gamma", "obs_model == 5", gamma_1dim__);
      current_statement__ = 190;
      validate_non_negative_index("eta", "num_comps", num_comps);
      current_statement__ = 191;
      validate_non_negative_index("eta", "num_obs", num_obs);
      current_statement__ = 192;
      validate_non_negative_index("f_latent", "num_comps", num_comps);
      current_statement__ = 193;
      validate_non_negative_index("f_latent", "num_obs", num_obs);
      current_statement__ = 194;
      teff_1dim__ = std::numeric_limits<int>::min();
      
      current_statement__ = 194;
      teff_1dim__ = logical_gt(num_uncrt, 0);
      current_statement__ = 194;
      validate_non_negative_index("teff", "num_uncrt > 0", teff_1dim__);
      current_statement__ = 195;
      validate_non_negative_index("teff", "num_bt", num_bt);
    } catch (const std::exception& e) {
      stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
    }
    num_params_r__ = 0U;
    
    try {
      num_params_r__ += num_comps;
      num_params_r__ += num_ell;
      num_params_r__ += num_ns;
      num_params_r__ += beta_1dim__ * num_bt;
      num_params_r__ += teff_raw_1dim__ * num_bt;
      num_params_r__ += sigma_1dim__;
      num_params_r__ += phi_1dim__;
      num_params_r__ += gamma_1dim__;
      num_params_r__ += num_comps * num_obs;
    } catch (const std::exception& e) {
      stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
    }
  }
  template <bool propto__, bool jacobian__, typename VecR, typename VecI, stan::require_vector_like_t<VecR>* = nullptr, stan::require_vector_like_vt<std::is_integral, VecI>* = nullptr>
  inline stan::scalar_type_t<VecR> log_prob_impl(VecR& params_r__,
                                                 VecI& params_i__,
                                                 std::ostream* pstream__ = nullptr) const {
    using T__ = stan::scalar_type_t<VecR>;
    using local_scalar_t__ = T__;
    T__ lp__(0.0);
    stan::math::accumulator<T__> lp_accum__;
    static const char* function__ = "model_lgp_latent_namespace::log_prob";
(void) function__;  // suppress unused var warning
    stan::io::reader<local_scalar_t__> in__(params_r__, params_i__);
    local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
    (void) DUMMY_VAR__;  // suppress unused var warning
    
    try {
      std::vector<local_scalar_t__> alpha;
      alpha = std::vector<local_scalar_t__>(num_comps, DUMMY_VAR__);
      
      current_statement__ = 1;
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        current_statement__ = 1;
        assign(alpha, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable alpha");}
      current_statement__ = 1;
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        current_statement__ = 1;
        if (jacobian__) {
          current_statement__ = 1;
          assign(alpha, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lb_constrain(alpha[(sym1__ - 1)], 1e-12, lp__),
            "assigning variable alpha");
        } else {
          current_statement__ = 1;
          assign(alpha, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lb_constrain(alpha[(sym1__ - 1)], 1e-12),
            "assigning variable alpha");
        }}
      std::vector<local_scalar_t__> ell;
      ell = std::vector<local_scalar_t__>(num_ell, DUMMY_VAR__);
      
      current_statement__ = 2;
      for (int sym1__ = 1; sym1__ <= num_ell; ++sym1__) {
        current_statement__ = 2;
        assign(ell, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable ell");}
      current_statement__ = 2;
      for (int sym1__ = 1; sym1__ <= num_ell; ++sym1__) {
        current_statement__ = 2;
        if (jacobian__) {
          current_statement__ = 2;
          assign(ell, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lb_constrain(ell[(sym1__ - 1)], 1e-12, lp__),
            "assigning variable ell");
        } else {
          current_statement__ = 2;
          assign(ell, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lb_constrain(ell[(sym1__ - 1)], 1e-12),
            "assigning variable ell");
        }}
      std::vector<local_scalar_t__> wrp;
      wrp = std::vector<local_scalar_t__>(num_ns, DUMMY_VAR__);
      
      current_statement__ = 3;
      for (int sym1__ = 1; sym1__ <= num_ns; ++sym1__) {
        current_statement__ = 3;
        assign(wrp, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable wrp");}
      current_statement__ = 3;
      for (int sym1__ = 1; sym1__ <= num_ns; ++sym1__) {
        current_statement__ = 3;
        if (jacobian__) {
          current_statement__ = 3;
          assign(wrp, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lb_constrain(wrp[(sym1__ - 1)], 1e-12, lp__),
            "assigning variable wrp");
        } else {
          current_statement__ = 3;
          assign(wrp, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lb_constrain(wrp[(sym1__ - 1)], 1e-12),
            "assigning variable wrp");
        }}
      std::vector<Eigen::Matrix<local_scalar_t__, -1, 1>> beta;
      beta = std::vector<Eigen::Matrix<local_scalar_t__, -1, 1>>(beta_1dim__, Eigen::Matrix<local_scalar_t__, -1, 1>(num_bt));
      stan::math::fill(beta, DUMMY_VAR__);
      
      current_statement__ = 4;
      for (int sym1__ = 1; sym1__ <= beta_1dim__; ++sym1__) {
        current_statement__ = 4;
        assign(beta, cons_list(index_uni(sym1__), nil_index_list()),
          in__.vector(num_bt), "assigning variable beta");}
      current_statement__ = 4;
      for (int sym1__ = 1; sym1__ <= beta_1dim__; ++sym1__) {
        current_statement__ = 4;
        for (int sym2__ = 1; sym2__ <= num_bt; ++sym2__) {
          current_statement__ = 4;
          if (jacobian__) {
            current_statement__ = 4;
            assign(beta,
              cons_list(index_uni(sym1__),
                cons_list(index_uni(sym2__), nil_index_list())),
              stan::math::lub_constrain(beta[(sym1__ - 1)][(sym2__ - 1)],
                1e-12, (1 - 1e-12), lp__), "assigning variable beta");
          } else {
            current_statement__ = 4;
            assign(beta,
              cons_list(index_uni(sym1__),
                cons_list(index_uni(sym2__), nil_index_list())),
              stan::math::lub_constrain(beta[(sym1__ - 1)][(sym2__ - 1)],
                1e-12, (1 - 1e-12)), "assigning variable beta");
          }}}
      std::vector<Eigen::Matrix<local_scalar_t__, -1, 1>> teff_raw;
      teff_raw = std::vector<Eigen::Matrix<local_scalar_t__, -1, 1>>(teff_raw_1dim__, Eigen::Matrix<local_scalar_t__, -1, 1>(num_bt));
      stan::math::fill(teff_raw, DUMMY_VAR__);
      
      current_statement__ = 5;
      for (int sym1__ = 1; sym1__ <= teff_raw_1dim__; ++sym1__) {
        current_statement__ = 5;
        assign(teff_raw, cons_list(index_uni(sym1__), nil_index_list()),
          in__.vector(num_bt), "assigning variable teff_raw");}
      current_statement__ = 5;
      for (int sym1__ = 1; sym1__ <= teff_raw_1dim__; ++sym1__) {
        current_statement__ = 5;
        for (int sym2__ = 1; sym2__ <= num_bt; ++sym2__) {
          current_statement__ = 5;
          if (jacobian__) {
            current_statement__ = 5;
            assign(teff_raw,
              cons_list(index_uni(sym1__),
                cons_list(index_uni(sym2__), nil_index_list())),
              stan::math::lub_constrain(teff_raw[(sym1__ - 1)][(sym2__ - 1)],
                1e-12, (1 - 1e-12), lp__), "assigning variable teff_raw");
          } else {
            current_statement__ = 5;
            assign(teff_raw,
              cons_list(index_uni(sym1__),
                cons_list(index_uni(sym2__), nil_index_list())),
              stan::math::lub_constrain(teff_raw[(sym1__ - 1)][(sym2__ - 1)],
                1e-12, (1 - 1e-12)), "assigning variable teff_raw");
          }}}
      std::vector<local_scalar_t__> sigma;
      sigma = std::vector<local_scalar_t__>(sigma_1dim__, DUMMY_VAR__);
      
      current_statement__ = 6;
      for (int sym1__ = 1; sym1__ <= sigma_1dim__; ++sym1__) {
        current_statement__ = 6;
        assign(sigma, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable sigma");}
      current_statement__ = 6;
      for (int sym1__ = 1; sym1__ <= sigma_1dim__; ++sym1__) {
        current_statement__ = 6;
        if (jacobian__) {
          current_statement__ = 6;
          assign(sigma, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lb_constrain(sigma[(sym1__ - 1)], 1e-12, lp__),
            "assigning variable sigma");
        } else {
          current_statement__ = 6;
          assign(sigma, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lb_constrain(sigma[(sym1__ - 1)], 1e-12),
            "assigning variable sigma");
        }}
      std::vector<local_scalar_t__> phi;
      phi = std::vector<local_scalar_t__>(phi_1dim__, DUMMY_VAR__);
      
      current_statement__ = 7;
      for (int sym1__ = 1; sym1__ <= phi_1dim__; ++sym1__) {
        current_statement__ = 7;
        assign(phi, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable phi");}
      current_statement__ = 7;
      for (int sym1__ = 1; sym1__ <= phi_1dim__; ++sym1__) {
        current_statement__ = 7;
        if (jacobian__) {
          current_statement__ = 7;
          assign(phi, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lb_constrain(phi[(sym1__ - 1)], 1e-12, lp__),
            "assigning variable phi");
        } else {
          current_statement__ = 7;
          assign(phi, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lb_constrain(phi[(sym1__ - 1)], 1e-12),
            "assigning variable phi");
        }}
      std::vector<local_scalar_t__> gamma;
      gamma = std::vector<local_scalar_t__>(gamma_1dim__, DUMMY_VAR__);
      
      current_statement__ = 8;
      for (int sym1__ = 1; sym1__ <= gamma_1dim__; ++sym1__) {
        current_statement__ = 8;
        assign(gamma, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable gamma");}
      current_statement__ = 8;
      for (int sym1__ = 1; sym1__ <= gamma_1dim__; ++sym1__) {
        current_statement__ = 8;
        if (jacobian__) {
          current_statement__ = 8;
          assign(gamma, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lub_constrain(gamma[(sym1__ - 1)], 1e-12,
              (1 - 1e-12), lp__), "assigning variable gamma");
        } else {
          current_statement__ = 8;
          assign(gamma, cons_list(index_uni(sym1__), nil_index_list()),
            stan::math::lub_constrain(gamma[(sym1__ - 1)], 1e-12, (1 - 1e-12)),
            "assigning variable gamma");
        }}
      std::vector<Eigen::Matrix<local_scalar_t__, -1, 1>> eta;
      eta = std::vector<Eigen::Matrix<local_scalar_t__, -1, 1>>(num_comps, Eigen::Matrix<local_scalar_t__, -1, 1>(num_obs));
      stan::math::fill(eta, DUMMY_VAR__);
      
      current_statement__ = 9;
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        current_statement__ = 9;
        assign(eta, cons_list(index_uni(sym1__), nil_index_list()),
          in__.vector(num_obs), "assigning variable eta");}
      std::vector<Eigen::Matrix<local_scalar_t__, -1, 1>> f_latent;
      f_latent = std::vector<Eigen::Matrix<local_scalar_t__, -1, 1>>(num_comps, Eigen::Matrix<local_scalar_t__, -1, 1>(num_obs));
      stan::math::fill(f_latent, DUMMY_VAR__);
      
      std::vector<Eigen::Matrix<local_scalar_t__, -1, 1>> teff;
      teff = std::vector<Eigen::Matrix<local_scalar_t__, -1, 1>>(teff_1dim__, Eigen::Matrix<local_scalar_t__, -1, 1>(num_bt));
      stan::math::fill(teff, DUMMY_VAR__);
      
      current_statement__ = 14;
      for (int j = 1; j <= num_uncrt; ++j) {
        current_statement__ = 12;
        assign(teff, cons_list(index_uni(j), nil_index_list()),
          add(teff_lb[(j - 1)],
            elt_multiply(subtract(teff_ub[(j - 1)], teff_lb[(j - 1)]),
              teff_raw[(j - 1)])), "assigning variable teff");}
      {
        current_statement__ = 15;
        validate_non_negative_index("Delta", "num_obs", num_obs);
        current_statement__ = 16;
        validate_non_negative_index("Delta", "num_obs", num_obs);
        Eigen::Matrix<local_scalar_t__, -1, -1> Delta;
        Delta = Eigen::Matrix<local_scalar_t__, -1, -1>(num_obs, num_obs);
        stan::math::fill(Delta, DUMMY_VAR__);
        
        current_statement__ = 17;
        assign(Delta, nil_index_list(), diag_matrix(delta_vec),
          "assigning variable Delta");
        current_statement__ = 18;
        validate_non_negative_index("KX", "num_comps", num_comps);
        current_statement__ = 19;
        validate_non_negative_index("KX", "num_obs", num_obs);
        current_statement__ = 20;
        validate_non_negative_index("KX", "num_obs", num_obs);
        std::vector<Eigen::Matrix<local_scalar_t__, -1, -1>> KX;
        KX = std::vector<Eigen::Matrix<local_scalar_t__, -1, -1>>(num_comps, Eigen::Matrix<local_scalar_t__, -1, -1>(num_obs, num_obs));
        stan::math::fill(KX, DUMMY_VAR__);
        
        current_statement__ = 21;
        assign(KX, nil_index_list(),
          STAN_kernel_all(num_obs, num_obs, K_const, components, x_cont,
            x_cont, x_cont_unnorm, x_cont_unnorm, alpha, ell, wrp, beta,
            teff, vm_params, idx_expand, idx_expand, teff_zero, pstream__),
          "assigning variable KX");
        current_statement__ = 24;
        for (int j = 1; j <= num_comps; ++j) {
          current_statement__ = 22;
          assign(f_latent, cons_list(index_uni(j), nil_index_list()),
            multiply(cholesky_decompose(add(KX[(j - 1)], Delta)),
              eta[(j - 1)]), "assigning variable f_latent");}
      }
      {
        current_statement__ = 26;
        validate_non_negative_index("f_sum", "num_obs", num_obs);
        Eigen::Matrix<local_scalar_t__, -1, 1> f_sum;
        f_sum = Eigen::Matrix<local_scalar_t__, -1, 1>(num_obs);
        stan::math::fill(f_sum, DUMMY_VAR__);
        
        current_statement__ = 27;
        assign(f_sum, nil_index_list(),
          add(STAN_vectorsum(f_latent, num_obs, pstream__), c_hat),
          "assigning variable f_sum");
        current_statement__ = 30;
        for (int j = 1; j <= num_comps; ++j) {
          current_statement__ = 28;
          lp_accum__.add(
            STAN_log_prior(alpha[(j - 1)], prior_alpha[(j - 1)],
              hyper_alpha[(j - 1)], pstream__));}
        current_statement__ = 33;
        for (int j = 1; j <= num_ell; ++j) {
          current_statement__ = 31;
          lp_accum__.add(
            STAN_log_prior(ell[(j - 1)], prior_ell[(j - 1)],
              hyper_ell[(j - 1)], pstream__));}
        current_statement__ = 36;
        for (int j = 1; j <= num_ns; ++j) {
          current_statement__ = 34;
          lp_accum__.add(
            STAN_log_prior(wrp[(j - 1)], prior_wrp[(j - 1)],
              hyper_wrp[(j - 1)], pstream__));}
        current_statement__ = 39;
        for (int j = 1; j <= num_heter; ++j) {
          current_statement__ = 37;
          lp_accum__.add(
            beta_lpdf<false>(beta[(j - 1)], hyper_beta[(j - 1)][(1 - 1)],
              hyper_beta[(j - 1)][(2 - 1)]));}
        current_statement__ = 49;
        for (int j = 1; j <= num_uncrt; ++j) {
          int ptype;
          ptype = std::numeric_limits<int>::min();
          
          current_statement__ = 40;
          ptype = prior_teff[(1 - 1)][(1 - 1)];
          int is_backwards;
          is_backwards = std::numeric_limits<int>::min();
          
          current_statement__ = 41;
          is_backwards = prior_teff[(1 - 1)][(2 - 1)];
          local_scalar_t__ direction;
          direction = DUMMY_VAR__;
          
          current_statement__ = 42;
          direction = pow(-1.0, is_backwards);
          current_statement__ = 43;
          validate_non_negative_index("tx", "num_bt", num_bt);
          Eigen::Matrix<local_scalar_t__, -1, 1> tx;
          tx = Eigen::Matrix<local_scalar_t__, -1, 1>(num_bt);
          stan::math::fill(tx, DUMMY_VAR__);
          
          current_statement__ = 44;
          assign(tx, nil_index_list(),
            multiply(direction, subtract(teff[(1 - 1)], teff_zero[(1 - 1)])),
            "assigning variable tx");
          current_statement__ = 47;
          for (int k = 1; k <= num_bt; ++k) {
            current_statement__ = 45;
            lp_accum__.add(
              STAN_log_prior(tx[(k - 1)], std::vector<int>{ptype, 0},
                hyper_teff[(1 - 1)], pstream__));}}
        current_statement__ = 52;
        for (int j = 1; j <= num_comps; ++j) {
          current_statement__ = 50;
          lp_accum__.add(normal_lpdf<false>(eta[(j - 1)], 0, 1));}
        current_statement__ = 61;
        if (logical_eq(obs_model, 1)) {
          current_statement__ = 59;
          lp_accum__.add(
            STAN_log_prior(sigma[(1 - 1)], prior_sigma[(1 - 1)],
              hyper_sigma[(1 - 1)], pstream__));
        } else {
          current_statement__ = 58;
          if (logical_eq(obs_model, 3)) {
            current_statement__ = 56;
            lp_accum__.add(
              STAN_log_prior(phi[(1 - 1)], prior_phi[(1 - 1)],
                hyper_phi[(1 - 1)], pstream__));
          } else {
            current_statement__ = 55;
            if (logical_eq(obs_model, 5)) {
              current_statement__ = 53;
              lp_accum__.add(
                beta_lpdf<false>(gamma[(1 - 1)],
                  hyper_gamma[(1 - 1)][(2 - 1)],
                  hyper_gamma[(1 - 1)][(2 - 1)]));
            } 
          }
        }
        current_statement__ = 93;
        if ((primitive_value(logical_eq(obs_model, 1)) && primitive_value(
            logical_eq(is_likelihood_skipped, 0)))) {
          current_statement__ = 89;
          validate_non_negative_index("MU", "num_obs", num_obs);
          std::vector<local_scalar_t__> MU;
          MU = std::vector<local_scalar_t__>(num_obs, DUMMY_VAR__);
          
          current_statement__ = 90;
          assign(MU, nil_index_list(), to_array_1d(f_sum),
            "assigning variable MU");
          current_statement__ = 91;
          lp_accum__.add(
            normal_lpdf<false>(y_real[(1 - 1)], MU, sigma[(1 - 1)]));
        } else {
          current_statement__ = 88;
          if ((primitive_value(logical_eq(obs_model, 2)) && primitive_value(
              logical_eq(is_likelihood_skipped, 0)))) {
            current_statement__ = 84;
            validate_non_negative_index("LOG_MU", "num_obs", num_obs);
            std::vector<local_scalar_t__> LOG_MU;
            LOG_MU = std::vector<local_scalar_t__>(num_obs, DUMMY_VAR__);
            
            current_statement__ = 85;
            assign(LOG_MU, nil_index_list(), to_array_1d(f_sum),
              "assigning variable LOG_MU");
            current_statement__ = 86;
            lp_accum__.add(poisson_log_lpmf<false>(y_int[(1 - 1)], LOG_MU));
          } else {
            current_statement__ = 83;
            if ((primitive_value(logical_eq(obs_model, 3)) &&
                primitive_value(logical_eq(is_likelihood_skipped, 0)))) {
              current_statement__ = 77;
              validate_non_negative_index("LOG_MU", "num_obs", num_obs);
              std::vector<local_scalar_t__> LOG_MU;
              LOG_MU = std::vector<local_scalar_t__>(num_obs, DUMMY_VAR__);
              
              current_statement__ = 78;
              assign(LOG_MU, nil_index_list(), to_array_1d(f_sum),
                "assigning variable LOG_MU");
              current_statement__ = 79;
              validate_non_negative_index("PHI", "num_obs", num_obs);
              std::vector<local_scalar_t__> PHI;
              PHI = std::vector<local_scalar_t__>(num_obs, DUMMY_VAR__);
              
              current_statement__ = 80;
              assign(PHI, nil_index_list(),
                to_array_1d(rep_vector(phi[(1 - 1)], num_obs)),
                "assigning variable PHI");
              current_statement__ = 81;
              lp_accum__.add(
                neg_binomial_2_log_lpmf<false>(y_int[(1 - 1)], LOG_MU, PHI));
            } else {
              current_statement__ = 76;
              if ((primitive_value(logical_eq(obs_model, 4)) &&
                  primitive_value(logical_eq(is_likelihood_skipped, 0)))) {
                current_statement__ = 72;
                validate_non_negative_index("LOGIT_P", "num_obs", num_obs);
                std::vector<local_scalar_t__> LOGIT_P;
                LOGIT_P = std::vector<local_scalar_t__>(num_obs, DUMMY_VAR__);
                
                current_statement__ = 73;
                assign(LOGIT_P, nil_index_list(), to_array_1d(f_sum),
                  "assigning variable LOGIT_P");
                current_statement__ = 74;
                lp_accum__.add(
                  binomial_logit_lpmf<false>(y_int[(1 - 1)],
                    y_num_trials[(1 - 1)], LOGIT_P));
              } else {
                current_statement__ = 71;
                if ((primitive_value(logical_eq(obs_model, 5)) &&
                    primitive_value(logical_eq(is_likelihood_skipped, 0)))) {
                  local_scalar_t__ tgam;
                  tgam = DUMMY_VAR__;
                  
                  current_statement__ = 62;
                  tgam = (inv(gamma[(1 - 1)]) - 1.0);
                  current_statement__ = 63;
                  validate_non_negative_index("P", "num_obs", num_obs);
                  Eigen::Matrix<local_scalar_t__, -1, 1> P;
                  P = Eigen::Matrix<local_scalar_t__, -1, 1>(num_obs);
                  stan::math::fill(P, DUMMY_VAR__);
                  
                  current_statement__ = 64;
                  assign(P, nil_index_list(), inv_logit(f_sum),
                    "assigning variable P");
                  current_statement__ = 65;
                  validate_non_negative_index("aa", "num_obs", num_obs);
                  std::vector<local_scalar_t__> aa;
                  aa = std::vector<local_scalar_t__>(num_obs, DUMMY_VAR__);
                  
                  current_statement__ = 66;
                  assign(aa, nil_index_list(),
                    to_array_1d(multiply(P, tgam)), "assigning variable aa");
                  current_statement__ = 67;
                  validate_non_negative_index("bb", "num_obs", num_obs);
                  std::vector<local_scalar_t__> bb;
                  bb = std::vector<local_scalar_t__>(num_obs, DUMMY_VAR__);
                  
                  current_statement__ = 68;
                  assign(bb, nil_index_list(),
                    to_array_1d(multiply(subtract(1.0, P), tgam)),
                    "assigning variable bb");
                  current_statement__ = 69;
                  lp_accum__.add(
                    beta_binomial_lpmf<false>(y_int[(1 - 1)],
                      y_num_trials[(1 - 1)], aa, bb));
                } 
              }
            }
          }
        }
      }
    } catch (const std::exception& e) {
      stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
    }
    lp_accum__.add(lp__);
    return lp_accum__.sum();
    } // log_prob_impl() 
    
  template <typename RNG, typename VecR, typename VecI, typename VecVar, stan::require_vector_like_vt<std::is_floating_point, VecR>* = nullptr, stan::require_vector_like_vt<std::is_integral, VecI>* = nullptr, stan::require_std_vector_vt<std::is_floating_point, VecVar>* = nullptr>
  inline void write_array_impl(RNG& base_rng__, VecR& params_r__,
                               VecI& params_i__, VecVar& vars__,
                               const bool emit_transformed_parameters__ = true,
                               const bool emit_generated_quantities__ = true,
                               std::ostream* pstream__ = nullptr) const {
    using local_scalar_t__ = double;
    vars__.resize(0);
    stan::io::reader<local_scalar_t__> in__(params_r__, params_i__);
    static const char* function__ = "model_lgp_latent_namespace::write_array";
(void) function__;  // suppress unused var warning
    (void) function__;  // suppress unused var warning
    double lp__ = 0.0;
    (void) lp__;  // dummy to suppress unused var warning
    stan::math::accumulator<double> lp_accum__;
    local_scalar_t__ DUMMY_VAR__(std::numeric_limits<double>::quiet_NaN());
    (void) DUMMY_VAR__;  // suppress unused var warning
    
    try {
      std::vector<double> alpha;
      alpha = std::vector<double>(num_comps, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 1;
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        current_statement__ = 1;
        assign(alpha, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable alpha");}
      current_statement__ = 1;
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        current_statement__ = 1;
        assign(alpha, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lb_constrain(alpha[(sym1__ - 1)], 1e-12),
          "assigning variable alpha");}
      std::vector<double> ell;
      ell = std::vector<double>(num_ell, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 2;
      for (int sym1__ = 1; sym1__ <= num_ell; ++sym1__) {
        current_statement__ = 2;
        assign(ell, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable ell");}
      current_statement__ = 2;
      for (int sym1__ = 1; sym1__ <= num_ell; ++sym1__) {
        current_statement__ = 2;
        assign(ell, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lb_constrain(ell[(sym1__ - 1)], 1e-12),
          "assigning variable ell");}
      std::vector<double> wrp;
      wrp = std::vector<double>(num_ns, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 3;
      for (int sym1__ = 1; sym1__ <= num_ns; ++sym1__) {
        current_statement__ = 3;
        assign(wrp, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable wrp");}
      current_statement__ = 3;
      for (int sym1__ = 1; sym1__ <= num_ns; ++sym1__) {
        current_statement__ = 3;
        assign(wrp, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lb_constrain(wrp[(sym1__ - 1)], 1e-12),
          "assigning variable wrp");}
      std::vector<Eigen::Matrix<double, -1, 1>> beta;
      beta = std::vector<Eigen::Matrix<double, -1, 1>>(beta_1dim__, Eigen::Matrix<double, -1, 1>(num_bt));
      stan::math::fill(beta, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 4;
      for (int sym1__ = 1; sym1__ <= beta_1dim__; ++sym1__) {
        current_statement__ = 4;
        assign(beta, cons_list(index_uni(sym1__), nil_index_list()),
          in__.vector(num_bt), "assigning variable beta");}
      current_statement__ = 4;
      for (int sym1__ = 1; sym1__ <= beta_1dim__; ++sym1__) {
        current_statement__ = 4;
        for (int sym2__ = 1; sym2__ <= num_bt; ++sym2__) {
          current_statement__ = 4;
          assign(beta,
            cons_list(index_uni(sym1__),
              cons_list(index_uni(sym2__), nil_index_list())),
            stan::math::lub_constrain(beta[(sym1__ - 1)][(sym2__ - 1)],
              1e-12, (1 - 1e-12)), "assigning variable beta");}}
      std::vector<Eigen::Matrix<double, -1, 1>> teff_raw;
      teff_raw = std::vector<Eigen::Matrix<double, -1, 1>>(teff_raw_1dim__, Eigen::Matrix<double, -1, 1>(num_bt));
      stan::math::fill(teff_raw, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 5;
      for (int sym1__ = 1; sym1__ <= teff_raw_1dim__; ++sym1__) {
        current_statement__ = 5;
        assign(teff_raw, cons_list(index_uni(sym1__), nil_index_list()),
          in__.vector(num_bt), "assigning variable teff_raw");}
      current_statement__ = 5;
      for (int sym1__ = 1; sym1__ <= teff_raw_1dim__; ++sym1__) {
        current_statement__ = 5;
        for (int sym2__ = 1; sym2__ <= num_bt; ++sym2__) {
          current_statement__ = 5;
          assign(teff_raw,
            cons_list(index_uni(sym1__),
              cons_list(index_uni(sym2__), nil_index_list())),
            stan::math::lub_constrain(teff_raw[(sym1__ - 1)][(sym2__ - 1)],
              1e-12, (1 - 1e-12)), "assigning variable teff_raw");}}
      std::vector<double> sigma;
      sigma = std::vector<double>(sigma_1dim__, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 6;
      for (int sym1__ = 1; sym1__ <= sigma_1dim__; ++sym1__) {
        current_statement__ = 6;
        assign(sigma, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable sigma");}
      current_statement__ = 6;
      for (int sym1__ = 1; sym1__ <= sigma_1dim__; ++sym1__) {
        current_statement__ = 6;
        assign(sigma, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lb_constrain(sigma[(sym1__ - 1)], 1e-12),
          "assigning variable sigma");}
      std::vector<double> phi;
      phi = std::vector<double>(phi_1dim__, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 7;
      for (int sym1__ = 1; sym1__ <= phi_1dim__; ++sym1__) {
        current_statement__ = 7;
        assign(phi, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable phi");}
      current_statement__ = 7;
      for (int sym1__ = 1; sym1__ <= phi_1dim__; ++sym1__) {
        current_statement__ = 7;
        assign(phi, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lb_constrain(phi[(sym1__ - 1)], 1e-12),
          "assigning variable phi");}
      std::vector<double> gamma;
      gamma = std::vector<double>(gamma_1dim__, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 8;
      for (int sym1__ = 1; sym1__ <= gamma_1dim__; ++sym1__) {
        current_statement__ = 8;
        assign(gamma, cons_list(index_uni(sym1__), nil_index_list()),
          in__.scalar(), "assigning variable gamma");}
      current_statement__ = 8;
      for (int sym1__ = 1; sym1__ <= gamma_1dim__; ++sym1__) {
        current_statement__ = 8;
        assign(gamma, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lub_constrain(gamma[(sym1__ - 1)], 1e-12, (1 - 1e-12)),
          "assigning variable gamma");}
      std::vector<Eigen::Matrix<double, -1, 1>> eta;
      eta = std::vector<Eigen::Matrix<double, -1, 1>>(num_comps, Eigen::Matrix<double, -1, 1>(num_obs));
      stan::math::fill(eta, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 9;
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        current_statement__ = 9;
        assign(eta, cons_list(index_uni(sym1__), nil_index_list()),
          in__.vector(num_obs), "assigning variable eta");}
      std::vector<Eigen::Matrix<double, -1, 1>> f_latent;
      f_latent = std::vector<Eigen::Matrix<double, -1, 1>>(num_comps, Eigen::Matrix<double, -1, 1>(num_obs));
      stan::math::fill(f_latent, std::numeric_limits<double>::quiet_NaN());
      
      std::vector<Eigen::Matrix<double, -1, 1>> teff;
      teff = std::vector<Eigen::Matrix<double, -1, 1>>(teff_1dim__, Eigen::Matrix<double, -1, 1>(num_bt));
      stan::math::fill(teff, std::numeric_limits<double>::quiet_NaN());
      
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        vars__.emplace_back(alpha[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= num_ell; ++sym1__) {
        vars__.emplace_back(ell[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= num_ns; ++sym1__) {
        vars__.emplace_back(wrp[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
        for (int sym2__ = 1; sym2__ <= beta_1dim__; ++sym2__) {
          vars__.emplace_back(beta[(sym2__ - 1)][(sym1__ - 1)]);}}
      for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
        for (int sym2__ = 1; sym2__ <= teff_raw_1dim__; ++sym2__) {
          vars__.emplace_back(teff_raw[(sym2__ - 1)][(sym1__ - 1)]);}}
      for (int sym1__ = 1; sym1__ <= sigma_1dim__; ++sym1__) {
        vars__.emplace_back(sigma[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= phi_1dim__; ++sym1__) {
        vars__.emplace_back(phi[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= gamma_1dim__; ++sym1__) {
        vars__.emplace_back(gamma[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
        for (int sym2__ = 1; sym2__ <= num_comps; ++sym2__) {
          vars__.emplace_back(eta[(sym2__ - 1)][(sym1__ - 1)]);}}
      if (logical_negation((primitive_value(emit_transformed_parameters__) ||
            primitive_value(emit_generated_quantities__)))) {
        return ;
      } 
      current_statement__ = 14;
      for (int j = 1; j <= num_uncrt; ++j) {
        current_statement__ = 12;
        assign(teff, cons_list(index_uni(j), nil_index_list()),
          add(teff_lb[(j - 1)],
            elt_multiply(subtract(teff_ub[(j - 1)], teff_lb[(j - 1)]),
              teff_raw[(j - 1)])), "assigning variable teff");}
      {
        current_statement__ = 15;
        validate_non_negative_index("Delta", "num_obs", num_obs);
        current_statement__ = 16;
        validate_non_negative_index("Delta", "num_obs", num_obs);
        Eigen::Matrix<double, -1, -1> Delta;
        Delta = Eigen::Matrix<double, -1, -1>(num_obs, num_obs);
        stan::math::fill(Delta, std::numeric_limits<double>::quiet_NaN());
        
        current_statement__ = 17;
        assign(Delta, nil_index_list(), diag_matrix(delta_vec),
          "assigning variable Delta");
        current_statement__ = 18;
        validate_non_negative_index("KX", "num_comps", num_comps);
        current_statement__ = 19;
        validate_non_negative_index("KX", "num_obs", num_obs);
        current_statement__ = 20;
        validate_non_negative_index("KX", "num_obs", num_obs);
        std::vector<Eigen::Matrix<double, -1, -1>> KX;
        KX = std::vector<Eigen::Matrix<double, -1, -1>>(num_comps, Eigen::Matrix<double, -1, -1>(num_obs, num_obs));
        stan::math::fill(KX, std::numeric_limits<double>::quiet_NaN());
        
        current_statement__ = 21;
        assign(KX, nil_index_list(),
          STAN_kernel_all(num_obs, num_obs, K_const, components, x_cont,
            x_cont, x_cont_unnorm, x_cont_unnorm, alpha, ell, wrp, beta,
            teff, vm_params, idx_expand, idx_expand, teff_zero, pstream__),
          "assigning variable KX");
        current_statement__ = 24;
        for (int j = 1; j <= num_comps; ++j) {
          current_statement__ = 22;
          assign(f_latent, cons_list(index_uni(j), nil_index_list()),
            multiply(cholesky_decompose(add(KX[(j - 1)], Delta)),
              eta[(j - 1)]), "assigning variable f_latent");}
      }
      if (emit_transformed_parameters__) {
        for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
          for (int sym2__ = 1; sym2__ <= num_comps; ++sym2__) {
            vars__.emplace_back(f_latent[(sym2__ - 1)][(sym1__ - 1)]);}}
        for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
          for (int sym2__ = 1; sym2__ <= teff_1dim__; ++sym2__) {
            vars__.emplace_back(teff[(sym2__ - 1)][(sym1__ - 1)]);}}
      } 
      if (logical_negation(emit_generated_quantities__)) {
        return ;
      } 
    } catch (const std::exception& e) {
      stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
    }
    } // write_array_impl() 
    
  template <typename VecVar, typename VecI, stan::require_std_vector_t<VecVar>* = nullptr, stan::require_vector_like_vt<std::is_integral, VecI>* = nullptr>
  inline void transform_inits_impl(const stan::io::var_context& context__,
                                   VecI& params_i__, VecVar& vars__,
                                   std::ostream* pstream__ = nullptr) const {
    using local_scalar_t__ = double;
    vars__.clear();
    vars__.reserve(num_params_r__);
    
    try {
      int pos__;
      pos__ = std::numeric_limits<int>::min();
      
      pos__ = 1;
      std::vector<double> alpha;
      alpha = std::vector<double>(num_comps, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 1;
      assign(alpha, nil_index_list(), context__.vals_r("alpha"),
        "assigning variable alpha");
      std::vector<double> alpha_free__;
      alpha_free__ = std::vector<double>(num_comps, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 1;
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        current_statement__ = 1;
        assign(alpha_free__, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lb_free(alpha[(sym1__ - 1)], 1e-12),
          "assigning variable alpha_free__");}
      std::vector<double> ell;
      ell = std::vector<double>(num_ell, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 2;
      assign(ell, nil_index_list(), context__.vals_r("ell"),
        "assigning variable ell");
      std::vector<double> ell_free__;
      ell_free__ = std::vector<double>(num_ell, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 2;
      for (int sym1__ = 1; sym1__ <= num_ell; ++sym1__) {
        current_statement__ = 2;
        assign(ell_free__, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lb_free(ell[(sym1__ - 1)], 1e-12),
          "assigning variable ell_free__");}
      std::vector<double> wrp;
      wrp = std::vector<double>(num_ns, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 3;
      assign(wrp, nil_index_list(), context__.vals_r("wrp"),
        "assigning variable wrp");
      std::vector<double> wrp_free__;
      wrp_free__ = std::vector<double>(num_ns, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 3;
      for (int sym1__ = 1; sym1__ <= num_ns; ++sym1__) {
        current_statement__ = 3;
        assign(wrp_free__, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lb_free(wrp[(sym1__ - 1)], 1e-12),
          "assigning variable wrp_free__");}
      std::vector<Eigen::Matrix<double, -1, 1>> beta;
      beta = std::vector<Eigen::Matrix<double, -1, 1>>(beta_1dim__, Eigen::Matrix<double, -1, 1>(num_bt));
      stan::math::fill(beta, std::numeric_limits<double>::quiet_NaN());
      
      {
        std::vector<local_scalar_t__> beta_flat__;
        current_statement__ = 4;
        assign(beta_flat__, nil_index_list(), context__.vals_r("beta"),
          "assigning variable beta_flat__");
        current_statement__ = 4;
        pos__ = 1;
        current_statement__ = 4;
        for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
          current_statement__ = 4;
          for (int sym2__ = 1; sym2__ <= beta_1dim__; ++sym2__) {
            current_statement__ = 4;
            assign(beta,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              beta_flat__[(pos__ - 1)], "assigning variable beta");
            current_statement__ = 4;
            pos__ = (pos__ + 1);}}
      }
      std::vector<Eigen::Matrix<double, -1, 1>> beta_free__;
      beta_free__ = std::vector<Eigen::Matrix<double, -1, 1>>(beta_1dim__, Eigen::Matrix<double, -1, 1>(num_bt));
      stan::math::fill(beta_free__, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 4;
      for (int sym1__ = 1; sym1__ <= beta_1dim__; ++sym1__) {
        current_statement__ = 4;
        for (int sym2__ = 1; sym2__ <= num_bt; ++sym2__) {
          current_statement__ = 4;
          assign(beta_free__,
            cons_list(index_uni(sym1__),
              cons_list(index_uni(sym2__), nil_index_list())),
            stan::math::lub_free(beta[(sym1__ - 1)][(sym2__ - 1)], 1e-12,
              (1 - 1e-12)), "assigning variable beta_free__");}}
      std::vector<Eigen::Matrix<double, -1, 1>> teff_raw;
      teff_raw = std::vector<Eigen::Matrix<double, -1, 1>>(teff_raw_1dim__, Eigen::Matrix<double, -1, 1>(num_bt));
      stan::math::fill(teff_raw, std::numeric_limits<double>::quiet_NaN());
      
      {
        std::vector<local_scalar_t__> teff_raw_flat__;
        current_statement__ = 5;
        assign(teff_raw_flat__, nil_index_list(),
          context__.vals_r("teff_raw"), "assigning variable teff_raw_flat__");
        current_statement__ = 5;
        pos__ = 1;
        current_statement__ = 5;
        for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
          current_statement__ = 5;
          for (int sym2__ = 1; sym2__ <= teff_raw_1dim__; ++sym2__) {
            current_statement__ = 5;
            assign(teff_raw,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              teff_raw_flat__[(pos__ - 1)], "assigning variable teff_raw");
            current_statement__ = 5;
            pos__ = (pos__ + 1);}}
      }
      std::vector<Eigen::Matrix<double, -1, 1>> teff_raw_free__;
      teff_raw_free__ = std::vector<Eigen::Matrix<double, -1, 1>>(teff_raw_1dim__, Eigen::Matrix<double, -1, 1>(num_bt));
      stan::math::fill(teff_raw_free__, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 5;
      for (int sym1__ = 1; sym1__ <= teff_raw_1dim__; ++sym1__) {
        current_statement__ = 5;
        for (int sym2__ = 1; sym2__ <= num_bt; ++sym2__) {
          current_statement__ = 5;
          assign(teff_raw_free__,
            cons_list(index_uni(sym1__),
              cons_list(index_uni(sym2__), nil_index_list())),
            stan::math::lub_free(teff_raw[(sym1__ - 1)][(sym2__ - 1)], 1e-12,
              (1 - 1e-12)), "assigning variable teff_raw_free__");}}
      std::vector<double> sigma;
      sigma = std::vector<double>(sigma_1dim__, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 6;
      assign(sigma, nil_index_list(), context__.vals_r("sigma"),
        "assigning variable sigma");
      std::vector<double> sigma_free__;
      sigma_free__ = std::vector<double>(sigma_1dim__, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 6;
      for (int sym1__ = 1; sym1__ <= sigma_1dim__; ++sym1__) {
        current_statement__ = 6;
        assign(sigma_free__, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lb_free(sigma[(sym1__ - 1)], 1e-12),
          "assigning variable sigma_free__");}
      std::vector<double> phi;
      phi = std::vector<double>(phi_1dim__, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 7;
      assign(phi, nil_index_list(), context__.vals_r("phi"),
        "assigning variable phi");
      std::vector<double> phi_free__;
      phi_free__ = std::vector<double>(phi_1dim__, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 7;
      for (int sym1__ = 1; sym1__ <= phi_1dim__; ++sym1__) {
        current_statement__ = 7;
        assign(phi_free__, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lb_free(phi[(sym1__ - 1)], 1e-12),
          "assigning variable phi_free__");}
      std::vector<double> gamma;
      gamma = std::vector<double>(gamma_1dim__, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 8;
      assign(gamma, nil_index_list(), context__.vals_r("gamma"),
        "assigning variable gamma");
      std::vector<double> gamma_free__;
      gamma_free__ = std::vector<double>(gamma_1dim__, std::numeric_limits<double>::quiet_NaN());
      
      current_statement__ = 8;
      for (int sym1__ = 1; sym1__ <= gamma_1dim__; ++sym1__) {
        current_statement__ = 8;
        assign(gamma_free__, cons_list(index_uni(sym1__), nil_index_list()),
          stan::math::lub_free(gamma[(sym1__ - 1)], 1e-12, (1 - 1e-12)),
          "assigning variable gamma_free__");}
      std::vector<Eigen::Matrix<double, -1, 1>> eta;
      eta = std::vector<Eigen::Matrix<double, -1, 1>>(num_comps, Eigen::Matrix<double, -1, 1>(num_obs));
      stan::math::fill(eta, std::numeric_limits<double>::quiet_NaN());
      
      {
        std::vector<local_scalar_t__> eta_flat__;
        current_statement__ = 9;
        assign(eta_flat__, nil_index_list(), context__.vals_r("eta"),
          "assigning variable eta_flat__");
        current_statement__ = 9;
        pos__ = 1;
        current_statement__ = 9;
        for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
          current_statement__ = 9;
          for (int sym2__ = 1; sym2__ <= num_comps; ++sym2__) {
            current_statement__ = 9;
            assign(eta,
              cons_list(index_uni(sym2__),
                cons_list(index_uni(sym1__), nil_index_list())),
              eta_flat__[(pos__ - 1)], "assigning variable eta");
            current_statement__ = 9;
            pos__ = (pos__ + 1);}}
      }
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        vars__.emplace_back(alpha_free__[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= num_ell; ++sym1__) {
        vars__.emplace_back(ell_free__[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= num_ns; ++sym1__) {
        vars__.emplace_back(wrp_free__[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= beta_1dim__; ++sym1__) {
        for (int sym2__ = 1; sym2__ <= num_bt; ++sym2__) {
          vars__.emplace_back(beta_free__[(sym1__ - 1)][(sym2__ - 1)]);}}
      for (int sym1__ = 1; sym1__ <= teff_raw_1dim__; ++sym1__) {
        for (int sym2__ = 1; sym2__ <= num_bt; ++sym2__) {
          vars__.emplace_back(teff_raw_free__[(sym1__ - 1)][(sym2__ - 1)]);}}
      for (int sym1__ = 1; sym1__ <= sigma_1dim__; ++sym1__) {
        vars__.emplace_back(sigma_free__[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= phi_1dim__; ++sym1__) {
        vars__.emplace_back(phi_free__[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= gamma_1dim__; ++sym1__) {
        vars__.emplace_back(gamma_free__[(sym1__ - 1)]);}
      for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
        for (int sym2__ = 1; sym2__ <= num_obs; ++sym2__) {
          vars__.emplace_back(eta[(sym1__ - 1)][(sym2__ - 1)]);}}
    } catch (const std::exception& e) {
      stan::lang::rethrow_located(e, locations_array__[current_statement__]);
      // Next line prevents compiler griping about no return
      throw std::runtime_error("*** IF YOU SEE THIS, PLEASE REPORT A BUG ***"); 
    }
    } // transform_inits_impl() 
    
  inline void get_param_names(std::vector<std::string>& names__) const {
    
    names__.clear();
    names__.emplace_back("alpha");
    names__.emplace_back("ell");
    names__.emplace_back("wrp");
    names__.emplace_back("beta");
    names__.emplace_back("teff_raw");
    names__.emplace_back("sigma");
    names__.emplace_back("phi");
    names__.emplace_back("gamma");
    names__.emplace_back("eta");
    names__.emplace_back("f_latent");
    names__.emplace_back("teff");
    } // get_param_names() 
    
  inline void get_dims(std::vector<std::vector<size_t>>& dimss__) const {
    dimss__.clear();
    dimss__.emplace_back(std::vector<size_t>{static_cast<size_t>(num_comps)});
    
    dimss__.emplace_back(std::vector<size_t>{static_cast<size_t>(num_ell)});
    
    dimss__.emplace_back(std::vector<size_t>{static_cast<size_t>(num_ns)});
    
    dimss__.emplace_back(std::vector<size_t>{static_cast<size_t>(beta_1dim__)
                                             , static_cast<size_t>(num_bt)});
    
    dimss__.emplace_back(std::vector<size_t>{
                                             static_cast<size_t>(teff_raw_1dim__)
                                             , static_cast<size_t>(num_bt)});
    
    dimss__.emplace_back(std::vector<size_t>{
                                             static_cast<size_t>(sigma_1dim__)});
    
    dimss__.emplace_back(std::vector<size_t>{static_cast<size_t>(phi_1dim__)});
    
    dimss__.emplace_back(std::vector<size_t>{
                                             static_cast<size_t>(gamma_1dim__)});
    
    dimss__.emplace_back(std::vector<size_t>{static_cast<size_t>(num_comps),
                                             static_cast<size_t>(num_obs)});
    
    dimss__.emplace_back(std::vector<size_t>{static_cast<size_t>(num_comps),
                                             static_cast<size_t>(num_obs)});
    
    dimss__.emplace_back(std::vector<size_t>{static_cast<size_t>(teff_1dim__)
                                             , static_cast<size_t>(num_bt)});
    
    } // get_dims() 
    
  inline void constrained_param_names(
                                      std::vector<std::string>& param_names__,
                                      bool emit_transformed_parameters__ = true,
                                      bool emit_generated_quantities__ = true) const
    final {
    
    for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "alpha" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= num_ell; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "ell" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= num_ns; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "wrp" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
      {
        for (int sym2__ = 1; sym2__ <= beta_1dim__; ++sym2__) {
          {
            param_names__.emplace_back(std::string() + "beta" + '.' + std::to_string(sym2__) + '.' + std::to_string(sym1__));
          }}
      }}
    for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
      {
        for (int sym2__ = 1; sym2__ <= teff_raw_1dim__; ++sym2__) {
          {
            param_names__.emplace_back(std::string() + "teff_raw" + '.' + std::to_string(sym2__) + '.' + std::to_string(sym1__));
          }}
      }}
    for (int sym1__ = 1; sym1__ <= sigma_1dim__; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "sigma" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= phi_1dim__; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "phi" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= gamma_1dim__; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "gamma" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
      {
        for (int sym2__ = 1; sym2__ <= num_comps; ++sym2__) {
          {
            param_names__.emplace_back(std::string() + "eta" + '.' + std::to_string(sym2__) + '.' + std::to_string(sym1__));
          }}
      }}
    if (emit_transformed_parameters__) {
      for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
        {
          for (int sym2__ = 1; sym2__ <= num_comps; ++sym2__) {
            {
              param_names__.emplace_back(std::string() + "f_latent" + '.' + std::to_string(sym2__) + '.' + std::to_string(sym1__));
            }}
        }}
      for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
        {
          for (int sym2__ = 1; sym2__ <= teff_1dim__; ++sym2__) {
            {
              param_names__.emplace_back(std::string() + "teff" + '.' + std::to_string(sym2__) + '.' + std::to_string(sym1__));
            }}
        }}
    }
    
    if (emit_generated_quantities__) {
      
    }
    
    } // constrained_param_names() 
    
  inline void unconstrained_param_names(
                                        std::vector<std::string>& param_names__,
                                        bool emit_transformed_parameters__ = true,
                                        bool emit_generated_quantities__ = true) const
    final {
    
    for (int sym1__ = 1; sym1__ <= num_comps; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "alpha" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= num_ell; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "ell" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= num_ns; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "wrp" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
      {
        for (int sym2__ = 1; sym2__ <= beta_1dim__; ++sym2__) {
          {
            param_names__.emplace_back(std::string() + "beta" + '.' + std::to_string(sym2__) + '.' + std::to_string(sym1__));
          }}
      }}
    for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
      {
        for (int sym2__ = 1; sym2__ <= teff_raw_1dim__; ++sym2__) {
          {
            param_names__.emplace_back(std::string() + "teff_raw" + '.' + std::to_string(sym2__) + '.' + std::to_string(sym1__));
          }}
      }}
    for (int sym1__ = 1; sym1__ <= sigma_1dim__; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "sigma" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= phi_1dim__; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "phi" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= gamma_1dim__; ++sym1__) {
      {
        param_names__.emplace_back(std::string() + "gamma" + '.' + std::to_string(sym1__));
      }}
    for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
      {
        for (int sym2__ = 1; sym2__ <= num_comps; ++sym2__) {
          {
            param_names__.emplace_back(std::string() + "eta" + '.' + std::to_string(sym2__) + '.' + std::to_string(sym1__));
          }}
      }}
    if (emit_transformed_parameters__) {
      for (int sym1__ = 1; sym1__ <= num_obs; ++sym1__) {
        {
          for (int sym2__ = 1; sym2__ <= num_comps; ++sym2__) {
            {
              param_names__.emplace_back(std::string() + "f_latent" + '.' + std::to_string(sym2__) + '.' + std::to_string(sym1__));
            }}
        }}
      for (int sym1__ = 1; sym1__ <= num_bt; ++sym1__) {
        {
          for (int sym2__ = 1; sym2__ <= teff_1dim__; ++sym2__) {
            {
              param_names__.emplace_back(std::string() + "teff" + '.' + std::to_string(sym2__) + '.' + std::to_string(sym1__));
            }}
        }}
    }
    
    if (emit_generated_quantities__) {
      
    }
    
    } // unconstrained_param_names() 
    
  inline std::string get_constrained_sizedtypes() const {
    stringstream s__;
    s__ << "[{\"name\":\"alpha\",\"type\":{\"name\":\"array\",\"length\":" << num_comps << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"ell\",\"type\":{\"name\":\"array\",\"length\":" << num_ell << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"wrp\",\"type\":{\"name\":\"array\",\"length\":" << num_ns << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"beta\",\"type\":{\"name\":\"array\",\"length\":" << beta_1dim__ << ",\"element_type\":{\"name\":\"vector\",\"length\":" << num_bt << "}},\"block\":\"parameters\"},{\"name\":\"teff_raw\",\"type\":{\"name\":\"array\",\"length\":" << teff_raw_1dim__ << ",\"element_type\":{\"name\":\"vector\",\"length\":" << num_bt << "}},\"block\":\"parameters\"},{\"name\":\"sigma\",\"type\":{\"name\":\"array\",\"length\":" << sigma_1dim__ << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"phi\",\"type\":{\"name\":\"array\",\"length\":" << phi_1dim__ << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"gamma\",\"type\":{\"name\":\"array\",\"length\":" << gamma_1dim__ << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"eta\",\"type\":{\"name\":\"array\",\"length\":" << num_comps << ",\"element_type\":{\"name\":\"vector\",\"length\":" << num_obs << "}},\"block\":\"parameters\"},{\"name\":\"f_latent\",\"type\":{\"name\":\"array\",\"length\":" << num_comps << ",\"element_type\":{\"name\":\"vector\",\"length\":" << num_obs << "}},\"block\":\"transformed_parameters\"},{\"name\":\"teff\",\"type\":{\"name\":\"array\",\"length\":" << teff_1dim__ << ",\"element_type\":{\"name\":\"vector\",\"length\":" << num_bt << "}},\"block\":\"transformed_parameters\"}]";
    return s__.str();
    } // get_constrained_sizedtypes() 
    
  inline std::string get_unconstrained_sizedtypes() const {
    stringstream s__;
    s__ << "[{\"name\":\"alpha\",\"type\":{\"name\":\"array\",\"length\":" << num_comps << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"ell\",\"type\":{\"name\":\"array\",\"length\":" << num_ell << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"wrp\",\"type\":{\"name\":\"array\",\"length\":" << num_ns << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"beta\",\"type\":{\"name\":\"array\",\"length\":" << beta_1dim__ << ",\"element_type\":{\"name\":\"vector\",\"length\":" << num_bt << "}},\"block\":\"parameters\"},{\"name\":\"teff_raw\",\"type\":{\"name\":\"array\",\"length\":" << teff_raw_1dim__ << ",\"element_type\":{\"name\":\"vector\",\"length\":" << num_bt << "}},\"block\":\"parameters\"},{\"name\":\"sigma\",\"type\":{\"name\":\"array\",\"length\":" << sigma_1dim__ << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"phi\",\"type\":{\"name\":\"array\",\"length\":" << phi_1dim__ << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"gamma\",\"type\":{\"name\":\"array\",\"length\":" << gamma_1dim__ << ",\"element_type\":{\"name\":\"real\"}},\"block\":\"parameters\"},{\"name\":\"eta\",\"type\":{\"name\":\"array\",\"length\":" << num_comps << ",\"element_type\":{\"name\":\"vector\",\"length\":" << num_obs << "}},\"block\":\"parameters\"},{\"name\":\"f_latent\",\"type\":{\"name\":\"array\",\"length\":" << num_comps << ",\"element_type\":{\"name\":\"vector\",\"length\":" << num_obs << "}},\"block\":\"transformed_parameters\"},{\"name\":\"teff\",\"type\":{\"name\":\"array\",\"length\":" << teff_1dim__ << ",\"element_type\":{\"name\":\"vector\",\"length\":" << num_bt << "}},\"block\":\"transformed_parameters\"}]";
    return s__.str();
    } // get_unconstrained_sizedtypes() 
    
  
    // Begin method overload boilerplate
    template <typename RNG>
    inline void write_array(RNG& base_rng,
                            Eigen::Matrix<double,Eigen::Dynamic,1>& params_r,
                            Eigen::Matrix<double,Eigen::Dynamic,1>& vars,
                            const bool emit_transformed_parameters = true,
                            const bool emit_generated_quantities = true,
                            std::ostream* pstream = nullptr) const {
      std::vector<double> vars_vec(vars.size());
      std::vector<int> params_i;
      write_array_impl(base_rng, params_r, params_i, vars_vec,
          emit_transformed_parameters, emit_generated_quantities, pstream);
      vars.resize(vars_vec.size());
      for (int i = 0; i < vars.size(); ++i) {
        vars.coeffRef(i) = vars_vec[i];
      }
    }
    template <typename RNG>
    inline void write_array(RNG& base_rng, std::vector<double>& params_r,
                            std::vector<int>& params_i,
                            std::vector<double>& vars,
                            bool emit_transformed_parameters = true,
                            bool emit_generated_quantities = true,
                            std::ostream* pstream = nullptr) const {
      write_array_impl(base_rng, params_r, params_i, vars, emit_transformed_parameters, emit_generated_quantities, pstream);
    }
    template <bool propto__, bool jacobian__, typename T_>
    inline T_ log_prob(Eigen::Matrix<T_,Eigen::Dynamic,1>& params_r,
                       std::ostream* pstream = nullptr) const {
      Eigen::Matrix<int, -1, 1> params_i;
      return log_prob_impl<propto__, jacobian__>(params_r, params_i, pstream);
    }
    template <bool propto__, bool jacobian__, typename T__>
    inline T__ log_prob(std::vector<T__>& params_r,
                        std::vector<int>& params_i,
                        std::ostream* pstream = nullptr) const {
      return log_prob_impl<propto__, jacobian__>(params_r, params_i, pstream);
    }
  
    inline void transform_inits(const stan::io::var_context& context,
                         Eigen::Matrix<double, Eigen::Dynamic, 1>& params_r,
                         std::ostream* pstream = nullptr) const final {
      std::vector<double> params_r_vec(params_r.size());
      std::vector<int> params_i;
      transform_inits_impl(context, params_i, params_r_vec, pstream);
      params_r.resize(params_r_vec.size());
      for (int i = 0; i < params_r.size(); ++i) {
        params_r.coeffRef(i) = params_r_vec[i];
      }
    }
    inline void transform_inits(const stan::io::var_context& context,
                                std::vector<int>& params_i,
                                std::vector<double>& vars,
                                std::ostream* pstream = nullptr) const final {
      transform_inits_impl(context, params_i, vars, pstream);
    }        
};
}
using stan_model = model_lgp_latent_namespace::model_lgp_latent;
#ifndef USING_R
// Boilerplate
stan::model::model_base& new_model(
        stan::io::var_context& data_context,
        unsigned int seed,
        std::ostream* msg_stream) {
  stan_model* m = new stan_model(data_context, seed, msg_stream);
  return *m;
}
stan::math::profile_map& get_stan_profile_data() {
  return model_lgp_latent_namespace::profiles__;
}
#endif
#endif
